{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pI9gLRAHOY4a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from CompressionLibrary.environments import ModelCompressionSVDIntEnv\n",
        "from CompressionLibrary.custom_layers import ROIEmbedding\n",
        "from CompressionLibrary.utils import calculate_model_weights\n",
        "from CompressionLibrary.reward_functions import reward_MnasNet_penalty as calculate_reward\n",
        "from uuid import uuid4\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "from functools import partial\n",
        "from IPython.display import clear_output\n",
        "import scipy.signal as scignal\n",
        "%matplotlib inline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_names = ['fashion_mnist', 'kmnist']\n",
        "\n",
        "run_id = datetime.now().strftime('%Y-%m-%d-%H-%M%S-') + str(uuid4())\n",
        "\n",
        "data_path = \"G:\\\\Python projects\\\\ModelCompressionRL\\\\data\\\\\"\n",
        "\n",
        "agent_name = 'PPO_DISCRETE_MKI_GENERALIST'\n",
        "\n",
        "\n",
        "# Env variables\n",
        "training_state_set_source = 'validation'\n",
        "training_num_feature_maps = 128\n",
        "\n",
        "\n",
        "# Testing variables\n",
        "testing_state_set_source = 'test'\n",
        "testing_num_feature_maps = -1\n",
        "eval_n_samples = 1\n",
        "test_frequency_epochs = 20 # Test every 20 epochs.\n",
        "\n",
        "\n",
        "#PPO parameters\n",
        "clip_ratio = 0.2\n",
        "gamma = 0.99\n",
        "lam = 0.97\n",
        "policy_learning_rate = 1e-6\n",
        "value_function_learning_rate = 1e-6\n",
        "train_policy_iterations = 80\n",
        "train_value_iterations = 80\n",
        "target_kl = 0.01\n",
        "\n",
        "log_name = '-'.join(dataset_names)\n",
        "logging.basicConfig(level=logging.DEBUG, handlers=[\n",
        "    logging.FileHandler(data_path + f'logs\\\\{agent_name}_{log_name}.log', 'w+')],\n",
        "    format='%(asctime)s -%(levelname)s - %(funcName)s -  %(message)s')\n",
        "logging.root.setLevel(logging.DEBUG)\n",
        "\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.ERROR)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "\n",
        "exploration_filename = data_path + f'stats\\\\{agent_name}_training.csv'\n",
        "test_filename = data_path + f'stats\\\\{agent_name}_testing.csv'\n",
        "agents_path = data_path+'agents\\\\PPO\\\\{}\\\\{}_{}'.format(agent_name,agent_name, log_name)\n",
        "\n",
        "\n",
        "current_state = 'layer_input'\n",
        "next_state = 'layer_output'\n",
        "layer_name_list = ['conv2d_1',  'dense', 'dense_1']\n",
        "\n",
        "\n",
        "n_games_training = 5\n",
        "n_games_testing = 1\n",
        "\n",
        "replay_num_samples = len(layer_name_list) * n_games_training\n",
        "verbose = 0\n",
        "rl_iterations = 1000\n",
        "eval_n_samples = 5\n",
        "n_samples_mode = 256\n",
        "batch_size_per_replica = 32\n",
        "tuning_batch_size = 128\n",
        "tuning_mode = 'final'\n",
        "rl_batch_size = batch_size_per_replica\n",
        "tuning_epochs = 0\n",
        "strategy=None\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Dataset and creation of LeNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_model(dataset_name, train_ds, valid_ds):\n",
        "    checkpoint_path = data_path+ f\"models\\\\lenet_{dataset_name}\\\\cp.ckpt\"\n",
        "    optimizer = tf.keras.optimizers.Adam(1e-5)\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "    train_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "    input = tf.keras.layers.Input((28,28,1))\n",
        "    x = tf.keras.layers.Conv2D(6, (5,5), padding='SAME', activation='sigmoid', name='conv2d')(input)\n",
        "    x = tf.keras.layers.AveragePooling2D((2,2), strides=2, name='avg_pool_1')(x)\n",
        "    x = tf.keras.layers.Conv2D(16, (5,5), padding='VALID', activation='sigmoid', name='conv2d_1')(x)\n",
        "    x = tf.keras.layers.AveragePooling2D((2,2), strides=2, name='avg_pool_2')(x)\n",
        "    x = tf.keras.layers.Flatten(name='flatten')(x)\n",
        "    x = tf.keras.layers.Dense(120, activation='sigmoid', name='dense')(x)\n",
        "    x = tf.keras.layers.Dense(84, activation='sigmoid', name='dense_1')(x)\n",
        "    x = tf.keras.layers.Dense(10, activation='softmax', name='predictions')(x)\n",
        "\n",
        "    model = tf.keras.Model(input, x, name='LeNet')\n",
        "    model.compile(optimizer=optimizer, loss=loss_object,\n",
        "                    metrics=[train_metric])\n",
        "\n",
        "    try:\n",
        "        model.load_weights(checkpoint_path).expect_partial()\n",
        "    except:\n",
        "        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=True,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "        model.fit(train_ds,\n",
        "          epochs=3000,\n",
        "          validation_data=valid_ds,\n",
        "          callbacks=[cp_callback])\n",
        "\n",
        "    return model       \n",
        "\n",
        "def dataset_preprocessing(img, label):\n",
        "    img = tf.cast(img, tf.float32)\n",
        "    img = img/255.0\n",
        "    return img, label\n",
        "\n",
        "def load_dataset(dataset_name, batch_size=128):\n",
        "    splits, info = tfds.load(dataset_name, as_supervised=True, with_info=True, shuffle_files=True, \n",
        "                                split=['train[:80%]', 'train[80%:]','test'])\n",
        "\n",
        "    (train_examples, validation_examples, test_examples) = splits\n",
        "    num_examples = info.splits['train'].num_examples\n",
        "\n",
        "    num_classes = info.features['label'].num_classes\n",
        "    input_shape = info.features['image'].shape\n",
        "\n",
        "    input_shape = (28,28,1)\n",
        "\n",
        "    train_ds = train_examples.map(dataset_preprocessing, num_parallel_calls=tf.data.AUTOTUNE).shuffle(buffer_size=1000, reshuffle_each_iteration=True).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    valid_ds = validation_examples.map(dataset_preprocessing, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    test_ds = test_examples.map(dataset_preprocessing, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_ds, valid_ds, test_ds, input_shape, num_classes"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create Environments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"LeNet\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 28, 28, 6)         156       \n",
            "                                                                 \n",
            " avg_pool_1 (AveragePooling2  (None, 14, 14, 6)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " avg_pool_2 (AveragePooling2  (None, 5, 5, 16)         0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 400)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 120)               48120     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 84)                10164     \n",
            "                                                                 \n",
            " predictions (Dense)         (None, 10)                850       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 61,706\n",
            "Trainable params: 61,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Number of actions 11\n"
          ]
        }
      ],
      "source": [
        "def create_environments(dataset_names, num_feature_maps, state_set_source):\n",
        "    w_comprs = ['InsertDenseSVD'] \n",
        "    l_comprs = ['MLPCompression']\n",
        "    compressors_list = w_comprs +  l_comprs\n",
        "\n",
        "    parameters = {}\n",
        "    parameters['InsertDenseSVD'] = {'layer_name': None, 'percentage': None}\n",
        "    parameters['MLPCompression'] = {'layer_name': None, 'percentage': None}\n",
        "    environments = []\n",
        "    for dataset in dataset_names:\n",
        "        train_ds, valid_ds, test_ds, input_shape, _ = load_dataset(dataset, tuning_batch_size)\n",
        "        new_func = partial(create_model, dataset_name=dataset, train_ds=train_ds, valid_ds=valid_ds)\n",
        "        env = ModelCompressionSVDIntEnv(\n",
        "                reward_func=calculate_reward,\n",
        "                compressors_list=compressors_list, \n",
        "                create_model_func=new_func, \n",
        "                compr_params=parameters, \n",
        "                train_ds=train_ds, \n",
        "                validation_ds=valid_ds, \n",
        "                test_ds=test_ds, \n",
        "                layer_name_list=layer_name_list, \n",
        "                input_shape=input_shape, \n",
        "                tuning_batch_size=tuning_batch_size, \n",
        "                tuning_epochs=tuning_epochs,\n",
        "                get_state_from=state_set_source, \n",
        "                current_state_source=current_state, \n",
        "                next_state_source=next_state, \n",
        "                num_feature_maps=num_feature_maps, \n",
        "                verbose=verbose,\n",
        "                tuning_mode=tuning_mode,\n",
        "                strategy=strategy)\n",
        "\n",
        "        environments.append(env)\n",
        "\n",
        "    return environments\n",
        "\n",
        "envs = create_environments(dataset_names,num_feature_maps=training_num_feature_maps, state_set_source=training_state_set_source)\n",
        "test_envs = create_environments(dataset_names,num_feature_maps=testing_num_feature_maps, state_set_source=testing_state_set_source)\n",
        "\n",
        "conv_shape, dense_shape = envs[0].observation_space()\n",
        "action_space = envs[0].action_space()\n",
        "n_actions = len(action_space)\n",
        "\n",
        "print(conv_shape, dense_shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dwPqd8omOUtk"
      },
      "outputs": [],
      "source": [
        "def cumulative_discounted_rewards(rw, discount):\n",
        "    return scignal.lfilter([1], [1, float(-discount)], rw[::-1], axis=0)[::-1]\n",
        "\n",
        "class ReplayBufferMultipleDatasetsPPO(object):\n",
        "    def __init__(self, size, dataset_names, gamma=0.99, lam=0.95):\n",
        "        \n",
        "        self.dataset_names = dataset_names\n",
        "        self.gamma = gamma\n",
        "        self.lam = lam\n",
        "        self._maxsize = size\n",
        "\n",
        "        # Dictionary to get the index of each dataset.\n",
        "        self.dataset_dict = dict(zip(dataset_names,range(len(self.dataset_names))))\n",
        "\n",
        "        # Create variables to store data.\n",
        "        self.reset()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return np.sum(list(map(lambda x: len(self._states[x]), self.dataset_names)))\n",
        "        \n",
        "    def reset(self):\n",
        "        self._states = dict(zip(dataset_names, map(lambda x: [], dataset_names)))\n",
        "        num_datasets = len(self.dataset_names)\n",
        "        self._actions = np.zeros((num_datasets, self._maxsize), dtype=np.int32)\n",
        "        self._advantages = np.zeros((num_datasets, self._maxsize), dtype=np.float32)\n",
        "        self._rewards = np.zeros((num_datasets, self._maxsize), dtype=np.float32)\n",
        "        self._returns = np.zeros((num_datasets, self._maxsize), dtype=np.float32)\n",
        "        self._values = np.zeros((num_datasets, self._maxsize), dtype=np.float32)\n",
        "        self._logprobs = np.zeros((num_datasets, self._maxsize), dtype=np.float32)\n",
        "\n",
        "        self._next_idx = np.zeros(num_datasets, dtype=np.int32)\n",
        "        self._trajectory_idx = np.zeros(num_datasets, dtype=np.int32)\n",
        "        \n",
        "    def add(self, s, a, rw, value, logprobs, dataset_name):\n",
        "\n",
        "        dataset_index = self.dataset_dict[dataset_name]\n",
        "        \n",
        "        \n",
        "        self._states[dataset_name].append(s)\n",
        "        self._actions[dataset_index,self._next_idx[dataset_index]] = a\n",
        "        self._rewards[dataset_index,self._next_idx[dataset_index]] = rw\n",
        "        self._values[dataset_index,self._next_idx[dataset_index]] = value\n",
        "        self._logprobs[dataset_index,self._next_idx[dataset_index]] = logprobs\n",
        "\n",
        "        self._next_idx[dataset_index] = (self._next_idx[dataset_index]+1) % self._maxsize\n",
        "\n",
        "    def finish_trajectory(self, dataset_name, last_value=0):\n",
        "        dataset_index = self.dataset_dict[dataset_name]\n",
        "\n",
        "        path_slice = slice(self._trajectory_idx[dataset_index], max(self._next_idx[dataset_index], len(self._states[dataset_name])))\n",
        "        \n",
        "        # Retrieve rewards and V(s)\n",
        "        rewards = np.append(self._rewards[dataset_index][path_slice], last_value)\n",
        "        values = np.append(self._values[dataset_index][path_slice], last_value)\n",
        "        # Calculate advantage and reward to go.\n",
        "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
        "\n",
        "        self._advantages[dataset_index,path_slice] = cumulative_discounted_rewards(deltas, self.gamma * self.lam)\n",
        "        self._returns[dataset_index,path_slice] = cumulative_discounted_rewards(rewards, self.gamma)[:-1]\n",
        "\n",
        "        self._trajectory_idx[dataset_index] = self._next_idx[dataset_index]\n",
        "   \n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        # Empty list for states\n",
        "        s = []\n",
        "        actions = []\n",
        "        advantages = []\n",
        "        returns = []\n",
        "        logprobs = []\n",
        "\n",
        "        num_datasets = len(self.dataset_names)\n",
        "        batch_counter = 0\n",
        "        recommended_batch_size = batch_size//num_datasets\n",
        "        for dataset_name in self._states.keys():\n",
        "            if batch_counter + recommended_batch_size > batch_size:\n",
        "                recommended_batch_size = batch_size - batch_counter\n",
        "\n",
        "            if recommended_batch_size < len(self._states[dataset_name]):\n",
        "                batch = np.random.choice(len(self._states[dataset_name]), recommended_batch_size, replace=False)\n",
        "                batch_counter += recommended_batch_size\n",
        "            else:\n",
        "                num_storage_sampes = len(self._states[dataset_name])\n",
        "                batch = np.random.choice(len(self._states[dataset_name]), num_storage_sampes, replace=False)\n",
        "                batch_counter += num_storage_sampes\n",
        "\n",
        "\n",
        "            for batch_element in batch:\n",
        "                # Remove  dimensions of size 1 so that it can be stacked.\n",
        "                s.append(tf.squeeze(self._states[dataset_name][batch_element]))\n",
        "\n",
        "            dataset_index = self.dataset_dict[dataset_name]\n",
        "            actions.extend(self._actions[dataset_index][batch])\n",
        "            advantages.extend(self._advantages[dataset_index][batch])\n",
        "            returns.extend(self._returns[dataset_index][batch])\n",
        "            logprobs.extend(self._logprobs[dataset_index][batch])\n",
        "\n",
        "\n",
        "        # Stack feature maps and add depth of 1.\n",
        "        s = tf.expand_dims(tf.ragged.stack(s), axis=-1)\n",
        "        return (s.to_tensor(),\n",
        "                tf.convert_to_tensor(actions, dtype=tf.int32),\n",
        "                tf.convert_to_tensor(advantages, dtype=tf.float32),\n",
        "                tf.convert_to_tensor(returns, dtype=tf.float32),\n",
        "                tf.convert_to_tensor(logprobs, dtype=tf.float32))\n",
        "    \n",
        "    \n",
        "exp_replay = ReplayBufferMultipleDatasetsPPO(replay_num_samples, dataset_names)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8D9yA82oUcY",
        "outputId": "fe6c70f1-7ae7-4785-8dac-710be46d2893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"PPOactor\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, None, None, 1)]   0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, None, None, 64)    640       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, None, None, 64)    36928     \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, None, None, 64)    36928     \n",
            "                                                                 \n",
            " roi_embedding (ROIEmbedding  (None, 5440)             0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               1392896   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 11)                2827      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,470,219\n",
            "Trainable params: 1,470,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"critic\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, None, None, 1)]   0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, None, None, 64)    640       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, None, None, 64)    36928     \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, None, None, 64)    36928     \n",
            "                                                                 \n",
            " roi_embedding (ROIEmbedding  (None, 5440)             0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               1392896   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,467,649\n",
            "Trainable params: 1,467,649\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "class PPOAgentConv:\n",
        "    def __init__(self, name, input_channels, n_actions):\n",
        "        \"\"\"A simple DQN agent\"\"\"\n",
        "\n",
        "        frames = tf.keras.layers.Input(shape=(None, None, input_channels))\n",
        "        x = tf.keras.layers.Conv2D(64, 3, strides=1, activation='relu')(frames)\n",
        "        x = tf.keras.layers.Conv2D(64, 3, strides=1, activation='relu')(x)\n",
        "        x = tf.keras.layers.Conv2D(64, 3, strides=1, activation='relu')(x)\n",
        "        x = ROIEmbedding(n_bins=[(8,8),(4,4), (2,2), (1,1)])(x)\n",
        "        c = tf.keras.layers.Dense(256, activation='tanh')(x)\n",
        "        c = tf.keras.layers.Dense(256, activation='tanh')(c)\n",
        "        critic_output = tf.keras.layers.Dense(1, activation=None)(c)\n",
        "        a = tf.keras.layers.Dense(256, activation='tanh')(x)\n",
        "        a = tf.keras.layers.Dense(256, activation='tanh')(a)\n",
        "        actor_output =  tf.keras.layers.Dense(n_actions, activation=None)(a)\n",
        "    \n",
        "        self.actor = tf.keras.Model(inputs=frames, outputs=actor_output, name=name+'actor')\n",
        "        self.critic = tf.keras.Model(inputs=frames, outputs=critic_output, name='critic')\n",
        "        self.n_actions = n_actions\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "    def sample_actions(self, states, greedy=False):\n",
        "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
        "        logits = self.actor(states)\n",
        "        self.logger.debug(f'Logits are {logits}')\n",
        "        softmax_prob = tf.nn.softmax(logits)\n",
        "        self.logger.debug(f'Softmax probabilities are {softmax_prob}.')\n",
        "        if greedy:\n",
        "            action = tf.math.argmax(softmax_prob, axis=-1)\n",
        "            self.logger.debug(f'Chosen action due to high probability was {action}.')\n",
        "        else:\n",
        "            action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
        "            self.logger.debug(f'Random chosen action was {action}.')\n",
        "        return logits, action\n",
        "\n",
        "class PPOAgentFC:\n",
        "    def __init__(self, name, input_channels, n_actions):\n",
        "        \"\"\"A simple DQN agent\"\"\"\n",
        "\n",
        "        frames = tf.keras.layers.Input(shape=(None, None, input_channels))\n",
        "        x = tf.keras.layers.Conv1D(64, 3, strides=1, activation='relu')(frames)\n",
        "        x = tf.keras.layers.Conv1D(64, 3, strides=1, activation='relu')(x)\n",
        "        x = tf.keras.layers.Conv1D(64, 3, strides=1, activation='relu')(x)\n",
        "        x = ROIEmbedding(n_bins=[32, 16, 8 ,4, 2, 1])(x)\n",
        "        c = tf.keras.layers.Dense(256, activation='tanh')(x)\n",
        "        c = tf.keras.layers.Dense(256, activation='tanh')(c)\n",
        "        critic_output = tf.keras.layers.Dense(1, activation=None)(c)\n",
        "        a = tf.keras.layers.Dense(256, activation='tanh')(x)\n",
        "        a = tf.keras.layers.Dense(256, activation='tanh')(a)\n",
        "        actor_output =  tf.keras.layers.Dense(n_actions, activation=None)(a)\n",
        "    \n",
        "        self.actor = tf.keras.Model(inputs=frames, outputs=actor_output, name=name+'actor')\n",
        "        self.critic = tf.keras.Model(inputs=frames, outputs=critic_output, name='critic')\n",
        "        self.n_actions = n_actions\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "    def sample_actions(self, states, greedy=False):\n",
        "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
        "        logits = self.actor(states)\n",
        "        self.logger.debug(f'Logits are {logits}')\n",
        "        softmax_prob = tf.nn.softmax(logits)\n",
        "        self.logger.debug(f'Softmax probabilities are {softmax_prob}.')\n",
        "        if greedy:\n",
        "            action = tf.math.argmax(softmax_prob, axis=-1)\n",
        "            self.logger.debug(f'Chosen action due to high probability was {action}.')\n",
        "        else:\n",
        "            action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
        "            self.logger.debug(f'Random chosen action was {action}.')\n",
        "        return logits, action\n",
        "    \n",
        "input_channels = conv_shape[-1]\n",
        "conv_agent = PPOAgentConv(name=\"PPO_conv\", input_channels=input_channels, n_actions=n_actions)\n",
        "fc_agent = PPOAgentFC(name=\"PPO_fc\", input_channels=input_channels, n_actions=n_actions)\n",
        "try:\n",
        "    conv_agent.actor.load_weights(agents_path+'actor.chkpt')\n",
        "    conv_agent.critic.load_weights(agents_path+'critic.chkpt')\n",
        "    fc_agent.actor.load_weights(agents_path+'actor.chkpt')\n",
        "    fc_agent.critic.load_weights(agents_path+'critic.chkpt')\n",
        "except:\n",
        "    print('No saved model was found.')\n",
        "print(conv_agent.actor.summary())\n",
        "print(conv_agent.critic.summary())\n",
        "print(fc_agent.actor.summary())\n",
        "print(fc_agent.critic.summary())\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sample generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KShOcf0KoeJ4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_log_prob(logits, a):\n",
        "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
        "    logprobabilities_all = tf.nn.log_softmax(logits)\n",
        "    logprobability = tf.reduce_sum(tf.one_hot(a, n_actions) * logprobabilities_all, axis=1)\n",
        "    return logprobability\n",
        "\n",
        "def play_and_record(env, run_id, test_number, dataset_name, save_name, n_games=1, save_replay=False, greedy=False):\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    # initial state\n",
        "    s = env.reset()\n",
        "    rewards = []\n",
        "    acc = []\n",
        "    weights = []\n",
        "    total_time = 0\n",
        "\n",
        "    # Play the game for n_steps as per instructions above\n",
        "    for game_id in range(n_games):\n",
        "        start = datetime.now()\n",
        "        for layer_number in range(1, len(env.layer_name_list)+1):\n",
        "            tf.keras.backend.clear_session()\n",
        "\n",
        "            # Get the current layer name\n",
        "            current_layer_name = env.layer_name_list[env._layer_counter]\n",
        "\n",
        "            # Get the layer.\n",
        "            layer = env.model.get_layer(current_layer_name)\n",
        "\n",
        "            was_conv = True\n",
        "            if isinstance(layer, tf.keras.layers.Conv2D):\n",
        "                # Calculate q values for batch of images\n",
        "                logits, action = conv_agent.sample_actions(s, greedy=greedy)\n",
        "            if isinstance(layer, tf.keras.layers.Dense):\n",
        "                was_conv = False\n",
        "                logits, action = fc_agent.sample_actions(s, greedy=greedy)\n",
        "            \n",
        "\n",
        "                \n",
        "\n",
        "            # Choose action\n",
        "            action = action[0].numpy()\n",
        "            logger.debug(f'Action for layer {current_layer_name} layer is {action}')\n",
        "            new_s, r, done, info = env.step(action)\n",
        "            logger.debug(f'Iteration {game_id} - Layer {current_layer_name} {layer_number}/{len(env.layer_name_list)}\\tChosen action {action} has {r} reward.')\n",
        "            logger.debug(info)\n",
        "            \n",
        "            new_s = env.get_state('current_state')\n",
        "            if new_s is None:\n",
        "                value_next_s = 0\n",
        "            else:\n",
        "\n",
        "                # Using fc agent as next state will always be fc.\n",
        "                value_next_s = fc_agent.critic(new_s)[0][0].numpy()\n",
        "            \n",
        "            if save_replay:\n",
        "                log_probability = get_log_prob(logits, action)[0]\n",
        "                exp_replay.add(s, action, r, value_next_s, log_probability, dataset_name)\n",
        "            \n",
        "            s = new_s\n",
        "\n",
        "            if done:\n",
        "                s = env.reset()\n",
        "                break\n",
        "\n",
        "        exp_replay.finish_trajectory(last_value=0, dataset_name=dataset_name)\n",
        "\n",
        "        actions = info['actions']\n",
        "        # Convert actions to str in one column.\n",
        "        info['actions'] = ','.join(map(str, actions))\n",
        "        info['run_id'] = run_id\n",
        "        info['test_number'] = test_number\n",
        "        info['game_id'] = game_id\n",
        "        info['dataset'] = dataset_name\n",
        "        del info['layer_name']\n",
        "        reward = info['reward']\n",
        "\n",
        "        rewards.append(reward)\n",
        "        acc.append(info['test_acc_after'])\n",
        "        weights.append(info['weights_after'])\n",
        "        new_row = pd.DataFrame(info, index=[0])\n",
        "        new_row.to_csv(save_name, mode='a', index=False)\n",
        "        end = datetime.now()\n",
        "        time_diff = (end - start).total_seconds()\n",
        "        total_time += time_diff\n",
        "        logger.info(f'Took {time_diff} seconds for one compression.')\n",
        "\n",
        "    logger.info(f'Evaluation of {n_games} took {total_time} secs. An average of {total_time/n_games} secs per game.')\n",
        "\n",
        "    return np.mean(rewards), np.mean(acc), np.mean(weights)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EFUmTR4eo4vm"
      },
      "outputs": [],
      "source": [
        "policy_optimizer = tf.keras.optimizers.Adam(policy_learning_rate, clipvalue=1.0)\n",
        "value_function_optimizer = tf.keras.optimizers.Adam(value_function_learning_rate,clipvalue=1.0)\n",
        "\n",
        "\n",
        "@tf.function#(experimental_relax_shapes=True)\n",
        "def train_policy(states, actions, log_probabilities, advantages):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = agent.actor(states)\n",
        "        ratio = tf.exp(get_log_prob(logits, actions)-log_probabilities)\n",
        "        min_advantage = tf.where(advantages>0, (1+clip_ratio)* advantages, (1-clip_ratio)* advantages )\n",
        "        new_adv = tf.minimum(ratio*advantages, min_advantage)\n",
        "        entropy = - tf.reduce_mean(tf.reduce_sum(tf.nn.softmax(logits) * tf.nn.log_softmax(logits), axis=1))\n",
        "        policy_loss = - tf.reduce_mean(new_adv)\n",
        "        loss = policy_loss + entropy\n",
        "    \n",
        "    gradients = tape.gradient(loss, agent.actor.trainable_weights)\n",
        "    policy_optimizer.apply_gradients(zip(gradients, agent.actor.trainable_weights))\n",
        "\n",
        "    kl = tf.reduce_mean(log_probabilities - get_log_prob(agent.actor(states), actions))\n",
        "    kl = tf.reduce_sum(kl)\n",
        "    return policy_loss, entropy, kl\n",
        "\n",
        "@tf.function#(experimental_relax_shapes=True)\n",
        "def train_value_function(states, returns):\n",
        "    with tf.GradientTape() as tape:\n",
        "        value_loss = tf.reduce_mean((returns - agent.critic(states))**2)\n",
        "\n",
        "    gradients = tape.gradient(value_loss, agent.critic.trainable_weights)\n",
        "    value_function_optimizer.apply_gradients(zip(gradients, agent.critic.trainable_weights))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "YA-Tppc4pGUl",
        "outputId": "0069800c-5aee-4183-a45b-c08acd4e4ddd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          |0/1000 [00:19<?, ?it/s, Last 3 RW: 0.00, 0.00 & 0.00 W: 0.00, 0.00 & 0.90 Acc: 0.00, 0.00 & 61706.00].\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m       dataset_name \u001b[39m=\u001b[39m dataset_names[idx]\n\u001b[0;32m     29\u001b[0m       logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mGenerating samples for dataset \u001b[39m\u001b[39m{\u001b[39;00mdataset_name\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m       rw, acc, weights \u001b[39m=\u001b[39m play_and_record(env, run_id\u001b[39m=\u001b[39;49mrun_id, test_number\u001b[39m=\u001b[39;49mi, dataset_name\u001b[39m=\u001b[39;49mdataset_name, save_name\u001b[39m=\u001b[39;49mexploration_filename, n_games\u001b[39m=\u001b[39;49mn_games_training, greedy\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  save_replay\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     31\u001b[0m       logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining stats for dataset \u001b[39m\u001b[39m{\u001b[39;00mdataset_name\u001b[39m}\u001b[39;00m\u001b[39m were rw:\u001b[39m\u001b[39m{\u001b[39;00mrw\u001b[39m}\u001b[39;00m\u001b[39m, acc:\u001b[39m\u001b[39m{\u001b[39;00macc\u001b[39m}\u001b[39;00m\u001b[39m, w:\u001b[39m\u001b[39m{\u001b[39;00mweights\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[39m# Train actor\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[11], line 26\u001b[0m, in \u001b[0;36mplay_and_record\u001b[1;34m(env, run_id, test_number, dataset_name, save_name, n_games, save_replay, greedy)\u001b[0m\n\u001b[0;32m     24\u001b[0m current_layer_name \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mlayer_name_list[env\u001b[39m.\u001b[39m_layer_counter]\n\u001b[0;32m     25\u001b[0m \u001b[39m# Choose action\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m logits, action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49msample_actions(s, greedy\u001b[39m=\u001b[39;49mgreedy)\n\u001b[0;32m     27\u001b[0m action \u001b[39m=\u001b[39m action[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     28\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAction for layer \u001b[39m\u001b[39m{\u001b[39;00mcurrent_layer_name\u001b[39m}\u001b[39;00m\u001b[39m layer is \u001b[39m\u001b[39m{\u001b[39;00maction\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
            "Cell \u001b[1;32mIn[10], line 23\u001b[0m, in \u001b[0;36mPPOAgent.sample_actions\u001b[1;34m(self, states, greedy)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msample_actions\u001b[39m(\u001b[39mself\u001b[39m, states, greedy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     22\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor(states)\n\u001b[0;32m     24\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLogits are \u001b[39m\u001b[39m{\u001b[39;00mlogits\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m     softmax_prob \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39msoftmax(logits)\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:557\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    555\u001b[0m     layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[1;32m--> 557\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1096\u001b[0m ):\n\u001b[1;32m-> 1097\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py:510\u001b[0m, in \u001b[0;36mFunctional.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[39m@doc_controls\u001b[39m\u001b[39m.\u001b[39mdo_not_doc_inheritable\n\u001b[0;32m    492\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs, training\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    493\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \n\u001b[0;32m    495\u001b[0m \u001b[39m    In this case `call` just reapplies\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[39m        a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 510\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_internal_graph(inputs, training\u001b[39m=\u001b[39;49mtraining, mask\u001b[39m=\u001b[39;49mmask)\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py:667\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    664\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[0;32m    666\u001b[0m args, kwargs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[1;32m--> 667\u001b[0m outputs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mlayer(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    669\u001b[0m \u001b[39m# Update tensor_dict.\u001b[39;00m\n\u001b[0;32m    670\u001b[0m \u001b[39mfor\u001b[39;00m x_id, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[0;32m    671\u001b[0m     node\u001b[39m.\u001b[39mflat_output_ids, tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(outputs)\n\u001b[0;32m    672\u001b[0m ):\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1094\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1096\u001b[0m ):\n\u001b[1;32m-> 1097\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "np.seterr(divide='ignore', invalid='ignore')\n",
        "\n",
        "\n",
        "num_datasets = len(dataset_names)\n",
        "\n",
        "num_tests = (rl_iterations//test_frequency_epochs) + 1\n",
        "\n",
        "weights_history_tests = np.zeros(shape=(num_tests, num_datasets))\n",
        "acc_history_tests = np.zeros(shape=(num_tests, num_datasets))\n",
        "rw_history_tests = np.zeros(shape=(num_tests, num_datasets))\n",
        "test_counter = 1\n",
        "\n",
        "policy_losses = np.zeros(rl_iterations*train_policy_iterations, dtype=np.float32)\n",
        "entropy_losses = np.zeros(rl_iterations*train_policy_iterations, dtype=np.float32)\n",
        "for idx, env in enumerate(envs):\n",
        "    weights_history_tests[0, idx ] = env.weights_before\n",
        "    acc_history_tests[0, idx] = env.test_acc_before\n",
        "\n",
        "\n",
        "highest_rw = 0\n",
        "\n",
        "\n",
        "with tqdm(total=rl_iterations,\n",
        "      bar_format=\"{l_bar}{bar}|{n}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}, Last 3 RW: {postfix[0][0]:.2f}, {postfix[0][1]:.2f} & {postfix[0][2]:.2f} W: {postfix[1][0]:.2f}, {postfix[1][1]:.2f} & {postfix[1][2]:.2f} Acc: {postfix[2][0]:.2f}, {postfix[2][1]:.2f} & {postfix[2][2]:.2f}].\",\n",
        "      postfix=[\n",
        "         dict({0: 0, 1: 0, 2: np.mean(rw_history_tests[0])}),\n",
        "         dict({0: 0, 1: 0, 2: np.mean(acc_history_tests[0])}),\n",
        "         dict({0: 0, 1: 0, 2: np.mean(weights_history_tests[0])})]) as t:\n",
        "    \n",
        "   for i in range(1, rl_iterations+1):\n",
        "      for idx, env in enumerate(envs):\n",
        "            dataset_name = dataset_names[idx]\n",
        "            logger.info(f'Generating samples for dataset {dataset_name}.')\n",
        "            rw, acc, weights = play_and_record(env, run_id=run_id, test_number=i, dataset_name=dataset_name, save_name=exploration_filename, n_games=n_games_training, greedy=False,  save_replay=True)\n",
        "            logger.info(f'Training stats for dataset {dataset_name} were rw:{rw}, acc:{acc}, w:{weights}.')\n",
        "            \n",
        "      # Train actor\n",
        "      for idx_policy_loss in range(train_policy_iterations):\n",
        "         states, actions, advantages, returns, logprobs = exp_replay.sample(rl_batch_size)\n",
        "         policy_loss, entropy, kl = train_policy(states=states, actions=actions, log_probabilities=logprobs, advantages=advantages)\n",
        "         entropy_losses[i*train_policy_iterations + idx_policy_loss] = entropy\n",
        "         policy_losses[i*train_policy_iterations + idx_policy_loss] = policy_loss\n",
        "         if kl > 1.5 * target_kl:\n",
        "            break\n",
        "      \n",
        "      # Train critic\n",
        "      for _ in range(train_value_iterations):\n",
        "         states, _, _, returns, _ = exp_replay.sample(rl_batch_size)\n",
        "         train_value_function(states=states, returns=returns)\n",
        "   \n",
        "\n",
        "      # Clear replay data.\n",
        "      exp_replay.reset()\n",
        "\n",
        "      if i % test_frequency_epochs == 0:\n",
        "         logger.info(f'Testing datasets.')\n",
        "         for idx, env in enumerate(test_envs):\n",
        "               dataset_name = dataset_names[idx]\n",
        "               logger.info(f'Testing agent for dataset {dataset_name}.')\n",
        "               rw, acc, weights = play_and_record(env, run_id=run_id, test_number=i, dataset_name=dataset_name,save_name=test_filename,n_games=n_games_testing, greedy=True, save_replay=False)\n",
        "               logger.info(f'Testing stats for dataset {dataset_name} were rw:{rw}, acc:{acc}, w:{weights}.')\n",
        "               rw_history_tests[test_counter, idx] = rw\n",
        "               acc_history_tests[test_counter, idx] = acc\n",
        "               weights_history_tests[test_counter, idx] = weights\n",
        "         \n",
        "\n",
        "         if np.mean(rw_history_tests[test_counter]) > highest_rw:\n",
        "            highest_rw = np.mean(rw_history_tests[test_counter])\n",
        "            agent.actor.save_weights(agents_path+'actor.chkpt')\n",
        "            agent.critic.save_weights(agents_path+'critic.chkpt')\n",
        "\n",
        "         t.postfix[0][2] = np.mean(rw_history_tests[test_counter])\n",
        "            \n",
        "            \n",
        "         try:\n",
        "               t.postfix[0][1] = np.mean(rw_history_tests[test_counter-1])\n",
        "         except IndexError:\n",
        "               t.postfix[0][1] = 0\n",
        "         try:\n",
        "               t.postfix[0][0] =  np.mean(rw_history_tests[test_counter-2])\n",
        "         except IndexError:\n",
        "               t.postfix[0][0] = 0\n",
        "\n",
        "         t.postfix[1][2] = np.mean(weights_history_tests[test_counter])\n",
        "         try:\n",
        "               t.postfix[1][1] = np.mean(weights_history_tests[test_counter-1])\n",
        "         except IndexError:\n",
        "               t.postfix[1][1] = 0\n",
        "         try:\n",
        "               t.postfix[1][0] = np.mean(weights_history_tests[test_counter-2])\n",
        "         except IndexError:\n",
        "               t.postfix[1][0] = 0\n",
        "\n",
        "         t.postfix[2][2] = np.mean(acc_history_tests[test_counter])\n",
        "         try:\n",
        "               t.postfix[2][1] = np.mean(acc_history_tests[test_counter-1])\n",
        "         except IndexError:\n",
        "               t.postfix[2][1] = 0\n",
        "         try:\n",
        "               t.postfix[2][0] = np.mean(acc_history_tests[test_counter-2])\n",
        "         except IndexError:\n",
        "               t.postfix[2][0] = 0\n",
        "\n",
        "         \n",
        "                    \n",
        "         #clear_output(True)\n",
        "\n",
        "         test_counter += 1\n",
        "         fig = plt.figure(figsize=(12,6))\n",
        "         ax1 = fig.add_subplot(131)\n",
        "         ax2 = fig.add_subplot(132)\n",
        "         ax3 = fig.add_subplot(133)\n",
        "         ax1.title.set_text('Accuracy')\n",
        "         for idx, dataset_name in enumerate(dataset_names):\n",
        "               ax1.plot(acc_history_tests[:test_counter, idx])\n",
        "         ax1.legend(dataset_names)\n",
        "         ax2.title.set_text('Weights')\n",
        "         for idx, dataset_name in enumerate(dataset_names):\n",
        "               ax2.plot(weights_history_tests[:test_counter, idx])\n",
        "         ax2.legend(dataset_names)\n",
        "         ax3.title.set_text('Reward')\n",
        "         for idx, dataset_name in enumerate(dataset_names):\n",
        "               ax3.plot(rw_history_tests[:test_counter, idx])\n",
        "         ax3.legend(dataset_names)\n",
        "         plt.xlabel('Epochs')\n",
        "         plt.savefig(data_path + f'figures\\\\{agent_name}.png', dpi=1200)\n",
        "         plt.close()\n",
        "         fig = plt.figure(figsize=(12,6))\n",
        "         plt.plot(entropy_losses[:i*train_policy_iterations + 1])\n",
        "         plt.plot(policy_losses[:i*train_policy_iterations + 1])\n",
        "         plt.legend(['Entropy loss', 'Policy loss'])\n",
        "         plt.xlabel('Epochs')\n",
        "         plt.savefig(data_path + f'figures\\\\{agent_name}_losses.png', dpi=1200)\n",
        "         plt.close()\n",
        "\n",
        "      t.update()\n",
        "    \n",
        "         "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    },
    "vscode": {
      "interpreter": {
        "hash": "47aaa0ff824686579d53327e8b8be0a557935a1d134cd2eb3e64085ff0644594"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
