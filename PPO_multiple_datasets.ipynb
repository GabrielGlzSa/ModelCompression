{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pI9gLRAHOY4a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from CompressionLibrary.environments import ModelCompressionSVDIntEnv\n",
        "from CompressionLibrary.custom_layers import ROIEmbedding\n",
        "from CompressionLibrary.utils import calculate_reward\n",
        "from uuid import uuid4\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "from functools import partial\n",
        "from IPython.display import clear_output\n",
        "import scipy.signal as scignal\n",
        "%matplotlib inline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_names = ['fashion_mnist', 'kmnist']\n",
        "\n",
        "run_id = datetime.now().strftime('%Y-%m-%d-%H-%M%S-') + str(uuid4())\n",
        "\n",
        "data_path = \"G:\\\\Python project\\\\MODEL COMPRESSION\\\\ModelCompressionRL\\\\data\\\\\"\n",
        "\n",
        "agent_name = 'PPO_DISCRETE_MKI_GENERALIST'\n",
        "\n",
        "test_frequency = 10\n",
        "\n",
        "test_counter = 0\n",
        "\n",
        "\n",
        "#PPO parameters\n",
        "clip_ratio = 0.2\n",
        "gamma = 0.99\n",
        "lam = 0.97\n",
        "policy_learning_rate = 1e-5\n",
        "value_function_learning_rate = 1e-5\n",
        "train_policy_iterations = 80\n",
        "train_value_iterations = 80\n",
        "target_kl = 0.01\n",
        "\n",
        "log_name = '-'.join(dataset_names)\n",
        "logging.basicConfig(level=logging.DEBUG, handlers=[\n",
        "    logging.FileHandler(data_path + f'logs\\\\{agent_name}_{log_name}.log', 'w+')],\n",
        "    format='%(asctime)s -%(levelname)s - %(funcName)s -  %(message)s')\n",
        "logging.root.setLevel(logging.DEBUG)\n",
        "\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.ERROR)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "\n",
        "exploration_filename = data_path + f'stats\\\\{agent_name}_training.csv'\n",
        "test_filename = data_path + f'stats\\\\{agent_name}_testing.csv'\n",
        "agents_path = data_path+'agents\\\\PPO\\\\{}\\\\{}_{}'.format(agent_name,agent_name, log_name)\n",
        "\n",
        "\n",
        "current_state = 'layer_weights'\n",
        "next_state = 'layer_weights'\n",
        "layer_name_list = ['conv2d_1',  'dense', 'dense_1']\n",
        "\n",
        "\n",
        "n_games_training = 5\n",
        "n_games_testing = 1\n",
        "\n",
        "replay_num_samples = len(layer_name_list) * n_games_training\n",
        "verbose = 0\n",
        "rl_iterations = 1000\n",
        "eval_n_samples = 5\n",
        "n_samples_mode = 256\n",
        "batch_size_per_replica = 32\n",
        "tuning_batch_size = 128\n",
        "tuning_mode = 'final'\n",
        "rl_batch_size = batch_size_per_replica\n",
        "tuning_epochs = 0\n",
        "strategy = None\n",
        "\n",
        "\n",
        "\n",
        "mean_rw_history = np.zeros(rl_iterations//test_frequency)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Dataset and creation of LeNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_model(dataset_name, train_ds, valid_ds):\n",
        "    checkpoint_path = f\"./data/models/lenet_{dataset_name}/cp.ckpt\"\n",
        "    optimizer = tf.keras.optimizers.Adam(1e-5)\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "    train_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "    input = tf.keras.layers.Input((28,28,1))\n",
        "    x = tf.keras.layers.Conv2D(6, (5,5), padding='SAME', activation='sigmoid', name='conv2d')(input)\n",
        "    x = tf.keras.layers.AveragePooling2D((2,2), strides=2, name='avg_pool_1')(x)\n",
        "    x = tf.keras.layers.Conv2D(16, (5,5), padding='VALID', activation='sigmoid', name='conv2d_1')(x)\n",
        "    x = tf.keras.layers.AveragePooling2D((2,2), strides=2, name='avg_pool_2')(x)\n",
        "    x = tf.keras.layers.Flatten(name='flatten')(x)\n",
        "    x = tf.keras.layers.Dense(120, activation='sigmoid', name='dense')(x)\n",
        "    x = tf.keras.layers.Dense(84, activation='sigmoid', name='dense_1')(x)\n",
        "    x = tf.keras.layers.Dense(10, activation='softmax', name='predictions')(x)\n",
        "\n",
        "    model = tf.keras.Model(input, x, name='LeNet')\n",
        "    model.compile(optimizer=optimizer, loss=loss_object,\n",
        "                    metrics=[train_metric])\n",
        "\n",
        "    try:\n",
        "        model.load_weights(checkpoint_path).expect_partial()\n",
        "    except:\n",
        "        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=True,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "        model.fit(train_ds,\n",
        "          epochs=3000,\n",
        "          validation_data=valid_ds,\n",
        "          callbacks=[cp_callback])\n",
        "\n",
        "    return model       \n",
        "\n",
        "def dataset_preprocessing(img, label):\n",
        "    img = tf.cast(img, tf.float32)\n",
        "    img = img/255.0\n",
        "    return img, label\n",
        "\n",
        "def load_dataset(dataset_name, batch_size=128):\n",
        "    splits, info = tfds.load(dataset_name, as_supervised=True, with_info=True, shuffle_files=True, \n",
        "                                split=['train[:80%]', 'train[80%:]','test'])\n",
        "\n",
        "    (train_examples, validation_examples, test_examples) = splits\n",
        "    num_examples = info.splits['train'].num_examples\n",
        "\n",
        "    num_classes = info.features['label'].num_classes\n",
        "    input_shape = info.features['image'].shape\n",
        "\n",
        "    input_shape = (28,28,1)\n",
        "\n",
        "    train_ds = train_examples.map(dataset_preprocessing, num_parallel_calls=tf.data.AUTOTUNE).shuffle(buffer_size=1000, reshuffle_each_iteration=True).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    valid_ds = validation_examples.map(dataset_preprocessing, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    test_ds = test_examples.map(dataset_preprocessing, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_ds, valid_ds, test_ds, input_shape, num_classes"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create Environments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"LeNet\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 28, 28, 6)         156       \n",
            "                                                                 \n",
            " avg_pool_1 (AveragePooling2  (None, 14, 14, 6)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " avg_pool_2 (AveragePooling2  (None, 5, 5, 16)         0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 400)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 120)               48120     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 84)                10164     \n",
            "                                                                 \n",
            " predictions (Dense)         (None, 10)                850       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 61,706\n",
            "Trainable params: 61,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def make_env(create_model, train_ds, valid_ds, test_ds, reward_func, input_shape, layer_name_list, num_feature_maps, tuning_batch_size, tuning_epochs, verbose=0, tuning_mode='final', current_state_source='layer_input', next_state_source='layer_output', strategy=None, model_path='./data'):\n",
        "\n",
        "    w_comprs = ['InsertDenseSVD'] \n",
        "    l_comprs = ['MLPCompression']\n",
        "    compressors_list = w_comprs +  l_comprs\n",
        "\n",
        "    parameters = {}\n",
        "    parameters['InsertDenseSVD'] = {'layer_name': None, 'percentage': None}\n",
        "    parameters['MLPCompression'] = {'layer_name': None, 'percentage': None}\n",
        "\n",
        "    env = ModelCompressionSVDIntEnv(compressors_list=compressors_list, \n",
        "                                    create_model_func=create_model, \n",
        "                                    compr_params=parameters, \n",
        "                                    train_ds=train_ds, \n",
        "                                    validation_ds=valid_ds, \n",
        "                                    test_ds=test_ds, \n",
        "                                    layer_name_list=layer_name_list, \n",
        "                                    input_shape=input_shape, \n",
        "                                    reward_func=reward_func,\n",
        "                                    tuning_batch_size=tuning_batch_size, \n",
        "                                    tuning_epochs=tuning_epochs, \n",
        "                                    tuning_mode=tuning_mode, \n",
        "                                    current_state_source=current_state_source, \n",
        "                                    next_state_source=next_state_source, \n",
        "                                    num_feature_maps=num_feature_maps, \n",
        "                                    verbose=verbose,\n",
        "                                    strategy=strategy, \n",
        "                                    model_path=model_path)\n",
        "\n",
        "    return env\n",
        "\n",
        "def create_environments(dataset_names):\n",
        "    environments = []\n",
        "    for dataset in dataset_names:\n",
        "        train_ds, valid_ds, test_ds, input_shape, _ = load_dataset(dataset, tuning_batch_size)\n",
        "        create_model_dataset = partial(create_model, dataset_name=dataset, train_ds=train_ds, valid_ds=valid_ds)\n",
        "        new_create_model_func = partial(create_model_dataset, dataset_name=dataset, train_ds=train_ds, valid_ds=valid_ds)\n",
        "\n",
        "        env = make_env(\n",
        "                create_model=new_create_model_func, \n",
        "                train_ds=train_ds, \n",
        "                valid_ds=valid_ds, \n",
        "                test_ds=test_ds, \n",
        "                input_shape=input_shape,\n",
        "                reward_func = calculate_reward,\n",
        "                layer_name_list=layer_name_list, \n",
        "                num_feature_maps=n_samples_mode,\n",
        "                tuning_batch_size=tuning_batch_size,\n",
        "                tuning_epochs = tuning_epochs, \n",
        "                verbose=verbose, \n",
        "                tuning_mode=tuning_mode, \n",
        "                current_state_source=current_state, \n",
        "                next_state_source=next_state, \n",
        "                strategy=strategy, \n",
        "                model_path=data_path)\n",
        "\n",
        "        environments.append(env)\n",
        "\n",
        "    return environments\n",
        "\n",
        "envs = create_environments(dataset_names)\n",
        "envs[0].model.summary()\n",
        "\n",
        "conv_shape, dense_shape = envs[0].observation_space()\n",
        "\n",
        "n_actions = len(envs[0].action_space())\n",
        "logger.debug(f'Number of actions {n_actions}')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dwPqd8omOUtk"
      },
      "outputs": [],
      "source": [
        "def cumulative_discounted_rewards(rw, discount):\n",
        "    return scignal.lfilter([1], [1, float(-discount)], rw[::-1], axis=0)[::-1]\n",
        "\n",
        "class ReplayBufferMultipleDatasetsPPO(object):\n",
        "    def __init__(self, size, dataset_names, gamma=0.99, lam=0.95):\n",
        "        \n",
        "        self.dataset_names = dataset_names\n",
        "        self.gamma = gamma\n",
        "        self.lam = lam\n",
        "        self._maxsize = size\n",
        "\n",
        "        # Dictionary to get the index of each dataset.\n",
        "        self.dataset_dict = dict(zip(dataset_names,range(len(self.dataset_names))))\n",
        "\n",
        "        # Create variables to store data.\n",
        "        self.reset()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return np.sum(list(map(lambda x: len(self._states[x]), self.dataset_names)))\n",
        "        \n",
        "    def reset(self):\n",
        "        self._states = dict(zip(dataset_names, map(lambda x: [], dataset_names)))\n",
        "        num_datasets = len(self.dataset_names)\n",
        "        self._actions = np.zeros((num_datasets, self._maxsize), dtype=np.int32)\n",
        "        self._advantages = np.zeros((num_datasets, self._maxsize), dtype=np.float32)\n",
        "        self._rewards = np.zeros((num_datasets, self._maxsize), dtype=np.float32)\n",
        "        self._returns = np.zeros((num_datasets, self._maxsize), dtype=np.float32)\n",
        "        self._values = np.zeros((num_datasets, self._maxsize), dtype=np.float32)\n",
        "        self._logprobs = np.zeros((num_datasets, self._maxsize), dtype=np.float32)\n",
        "\n",
        "        self._next_idx = np.zeros((num_datasets), dtype=np.int32)\n",
        "        self._trajectory_idx = np.zeros((num_datasets), dtype=np.int32)\n",
        "        \n",
        "    def add(self, s, a, rw, value, logprobs, dataset_name):\n",
        "\n",
        "        dataset_index = self.dataset_dict[dataset_name]\n",
        "        self._states[dataset_name].append(s)\n",
        "        self._actions[dataset_index][self._next_idx] = a\n",
        "        self._rewards[dataset_index][self._next_idx] = rw\n",
        "        self._values[dataset_index][self._next_idx] = value\n",
        "        self._logprobs[dataset_index][self._next_idx] = logprobs\n",
        "        self._next_idx[dataset_index] = (self._next_idx[dataset_index]+1) % (self._maxsize/len(self.dataset_names))\n",
        "\n",
        "\n",
        "    def finish_trajectory(self, dataset_name, last_value=0):\n",
        "        dataset_index = self.dataset_dict[dataset_name]\n",
        "\n",
        "        path_slice = slice(self._trajectory_idx[dataset_index], max(self._next_idx[dataset_index], len(self._states[dataset_name])))\n",
        "        \n",
        "        # Retrieve rewards and V(s)\n",
        "        rewards = np.append(self._rewards[dataset_index][path_slice], last_value)\n",
        "        values = np.append(self._values[dataset_index][path_slice], last_value)\n",
        "\n",
        "        # Calculate advantage and reward to go.\n",
        "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
        "        self._advantages[dataset_index][path_slice] = cumulative_discounted_rewards(deltas, self.gamma * self.lam)\n",
        "        self._returns[dataset_index][path_slice] = cumulative_discounted_rewards(rewards, self.gamma)[:-1]\n",
        "\n",
        "        self._trajectory_idx[dataset_index] = self._next_idx[dataset_index]\n",
        "    \n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        # Empty list for states\n",
        "        s = []\n",
        "        actions = []\n",
        "        advantages = []\n",
        "        returns = []\n",
        "        logprobs = []\n",
        "\n",
        "        num_datasets = len(self.dataset_names)\n",
        "        batch_counter = 0\n",
        "        recommended_batch_size = batch_size//num_datasets\n",
        "        for dataset_name in self._states.keys():\n",
        "            if batch_counter + recommended_batch_size > batch_size:\n",
        "                recommended_batch_size = batch_size - batch_counter\n",
        "\n",
        "            if recommended_batch_size < len(self._states[dataset_name]):\n",
        "                batch = np.random.choice(len(self._states[dataset_name]), recommended_batch_size, replace=False)\n",
        "                batch_counter += recommended_batch_size\n",
        "            else:\n",
        "                num_storage_sampes = len(self._states[dataset_name])\n",
        "                batch = np.random.choice(len(self._states[dataset_name]), num_storage_sampes, replace=False)\n",
        "                batch_counter += num_storage_sampes\n",
        "\n",
        "\n",
        "            for batch_element in batch:\n",
        "                # Remove  dimensions of size 1 so that it can be stacked.\n",
        "                s.append(tf.squeeze(self._states[dataset_name][batch_element]))\n",
        "\n",
        "            dataset_index = self.dataset_dict[dataset_name]\n",
        "            actions.extend(self._actions[dataset_index][batch])\n",
        "            advantages.extend(self._advantages[dataset_index][batch])\n",
        "            returns.extend(self._returns[dataset_index][batch])\n",
        "            logprobs.extend(self._logprobs[dataset_index][batch])\n",
        "\n",
        "\n",
        "        # Stack feature maps and add depth of 1.\n",
        "        s = tf.expand_dims(tf.ragged.stack(s), axis=-1)\n",
        "        return (s.to_tensor(),\n",
        "                tf.convert_to_tensor(actions, dtype=tf.int32),\n",
        "                tf.convert_to_tensor(advantages, dtype=tf.float32),\n",
        "                tf.convert_to_tensor(returns, dtype=tf.float32),\n",
        "                tf.convert_to_tensor(logprobs, dtype=tf.float32))\n",
        "    \n",
        "    \n",
        "exp_replay = ReplayBufferMultipleDatasetsPPO(replay_num_samples, dataset_names)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8D9yA82oUcY",
        "outputId": "fe6c70f1-7ae7-4785-8dac-710be46d2893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No saved model was found.\n",
            "Model: \"PPOactor\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, None, None, 1)]   0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, None, None, 64)    640       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, None, None, 64)    36928     \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, None, None, 64)    36928     \n",
            "                                                                 \n",
            " roi_embedding (ROIEmbedding  (None, 5440)             0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               1392896   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 11)                2827      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,470,219\n",
            "Trainable params: 1,470,219\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"critic\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, None, None, 1)]   0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, None, None, 64)    640       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, None, None, 64)    36928     \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, None, None, 64)    36928     \n",
            "                                                                 \n",
            " roi_embedding (ROIEmbedding  (None, 5440)             0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               1392896   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,467,649\n",
            "Trainable params: 1,467,649\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "class PPOAgent:\n",
        "    def __init__(self, name, input_channels, n_actions):\n",
        "        \"\"\"A simple DQN agent\"\"\"\n",
        "\n",
        "        frames = tf.keras.layers.Input(shape=(None, None, input_channels))\n",
        "        x = tf.keras.layers.Conv2D(64, 3, strides=1, activation='relu')(frames)\n",
        "        x = tf.keras.layers.Conv2D(64, 3, strides=1, activation='relu')(x)\n",
        "        x = tf.keras.layers.Conv2D(64, 3, strides=1, activation='relu')(x)\n",
        "        x = ROIEmbedding(n_bins=[(8,8),(4,4), (2,2), (1,1)])(x)\n",
        "        c = tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "        critic_output = tf.keras.layers.Dense(1, activation='linear')(c)\n",
        "        a = tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "        actor_output =  tf.keras.layers.Dense(n_actions, activation='linear')(a)\n",
        "    \n",
        "        self.actor = tf.keras.Model(inputs=frames, outputs=actor_output, name=name+'actor')\n",
        "        self.critic = tf.keras.Model(inputs=frames, outputs=critic_output, name='critic')\n",
        "        self.n_actions = n_actions\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "    def sample_actions(self, states, greedy=False):\n",
        "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
        "        logits = self.actor(states)\n",
        "        self.logger.debug(f'Logits are {logits}')\n",
        "        if greedy:\n",
        "            softmax_prob = tf.nn.softmax(logits)\n",
        "            self.logger.debug(f'Softmax probabilities are {softmax_prob}.')\n",
        "            action = tf.math.argmax(softmax_prob, axis=-1)\n",
        "            self.logger.debug(f'Chosen action due to high probability was {action}.')\n",
        "        else:\n",
        "            action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
        "            self.logger.debug(f'Random chosen action was {action}.')\n",
        "        return logits, action\n",
        "\n",
        "input_channels = 1\n",
        "agent = PPOAgent(name=\"PPO\", input_channels=input_channels, n_actions=n_actions)\n",
        "\n",
        "try:\n",
        "    agent.actor.load_weights(agents_path+'actor.chkpt')\n",
        "    agent.critic.load_weights(agents_path+'critic.chkpt')\n",
        "except:\n",
        "    print('No saved model was found.')\n",
        "print(agent.actor.summary())\n",
        "print(agent.critic.summary())\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sample generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KShOcf0KoeJ4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_log_prob(logits, a):\n",
        "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
        "    logprobabilities_all = tf.nn.log_softmax(logits)\n",
        "    logprobability = tf.reduce_sum(tf.one_hot(a, n_actions) * logprobabilities_all, axis=1)\n",
        "    return logprobability\n",
        "\n",
        "def play_and_record(env, run_id, test_number, dataset_name, save_name, n_games=1, save_replay=False, greedy=False):\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    # initial state\n",
        "    s = env.reset()\n",
        "    rewards = []\n",
        "    acc = []\n",
        "    weights = []\n",
        "    total_time = 0\n",
        "\n",
        "    # Play the game for n_steps as per instructions above\n",
        "    for game_id in range(n_games):\n",
        "        start = datetime.now()\n",
        "        for layer_number in range(1, len(env.layer_name_list)+1):\n",
        "            tf.keras.backend.clear_session()\n",
        "\n",
        "            # Get the current layer name\n",
        "            current_layer_name = env.layer_name_list[env._layer_counter]\n",
        "            # Choose action\n",
        "            logits, action = agent.sample_actions(s, greedy=greedy)\n",
        "            logger.debug(f'Action for layer {current_layer_name} layer is {action}')\n",
        "            new_s, r, done, info = env.step(action[0].numpy())\n",
        "            logger.debug(f'Iteration {game_id} - Layer {current_layer_name} {layer_number}/{len(env.layer_name_list)}\\tChosen action {action} has {r} reward.')\n",
        "            logger.debug(info)\n",
        "            \n",
        "            new_s = env.get_state('current_state')\n",
        "            if new_s is None:\n",
        "                value_next_s = 0\n",
        "            else:\n",
        "                value_next_s = agent.critic(new_s)\n",
        "            \n",
        "            if save_replay:\n",
        "                log_probability = get_log_prob(logits, action)\n",
        "                exp_replay.add(s, action, r, value_next_s, log_probability, dataset_name)\n",
        "            \n",
        "            s = new_s\n",
        "\n",
        "            if done:\n",
        "                s = env.reset()\n",
        "                break\n",
        "\n",
        "        exp_replay.finish_trajectory(last_value=0, dataset_name=dataset_name)\n",
        "\n",
        "        actions = info['actions']\n",
        "        # Convert actions to str in one column.\n",
        "        info['actions'] = ','.join(map(str, actions))\n",
        "        info['run_id'] = run_id\n",
        "        info['test_number'] = test_number\n",
        "        info['game_id'] = game_id\n",
        "        info['dataset'] = dataset_name\n",
        "        del info['layer_name']\n",
        "        reward = info['reward']\n",
        "\n",
        "        rewards.append(reward)\n",
        "        acc.append(info['test_acc_after'])\n",
        "        weights.append(info['weights_after'])\n",
        "        new_row = pd.DataFrame(info, index=[0])\n",
        "        new_row.to_csv(save_name, mode='a', index=False)\n",
        "        end = datetime.now()\n",
        "        time_diff = (end - start).total_seconds()\n",
        "        total_time += time_diff\n",
        "        logger.info(f'Took {time_diff} seconds for one compression.')\n",
        "\n",
        "    logger.info(f'Evaluation of {n_games} took {total_time} secs. An average of {total_time/n_games} secs per game.')\n",
        "\n",
        "    return np.mean(rewards), np.mean(acc), np.mean(weights)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EFUmTR4eo4vm"
      },
      "outputs": [],
      "source": [
        "policy_optimizer = tf.keras.optimizers.Adam(policy_learning_rate, clipvalue=1.0)\n",
        "value_function_optimizer = tf.keras.optimizers.Adam(value_function_learning_rate,clipvalue=1.0)\n",
        "\n",
        "\n",
        "@tf.function#(experimental_relax_shapes=True)\n",
        "def train_policy(states, actions, log_probabilities, advantages):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = agent.actor(states)\n",
        "        ratio = tf.exp(get_log_prob(logits, actions)-log_probabilities)\n",
        "        min_advantage = tf.where(advantages>0, (1+clip_ratio)* advantages, (1-clip_ratio)* advantages )\n",
        "        new_adv = tf.minimum(ratio*advantages, min_advantage)\n",
        "        entropy = - tf.reduce_sum(tf.nn.softmax(logits) * tf.nn.log_softmax(logits))\n",
        "        policy_loss = - tf.reduce_mean(new_adv) + entropy\n",
        "    \n",
        "    gradients = tape.gradient(policy_loss, agent.actor.trainable_weights)\n",
        "    policy_optimizer.apply_gradients(zip(gradients, agent.actor.trainable_weights))\n",
        "\n",
        "    kl = tf.reduce_mean(log_probabilities - get_log_prob(agent.actor(states), actions))\n",
        "    kl = tf.reduce_sum(kl)\n",
        "    return policy_loss, kl\n",
        "\n",
        "@tf.function#(experimental_relax_shapes=True)\n",
        "def train_value_function(states, returns):\n",
        "    with tf.GradientTape() as tape:\n",
        "        value_loss = tf.reduce_mean((returns - agent.critic(states))**2)\n",
        "\n",
        "    gradients = tape.gradient(value_loss, agent.critic.trainable_weights)\n",
        "    value_function_optimizer.apply_gradients(zip(gradients, agent.critic.trainable_weights))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "YA-Tppc4pGUl",
        "outputId": "0069800c-5aee-4183-a45b-c08acd4e4ddd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|‚ñè         |17/1000 [1:08:27<65:58:53, 241.64s/it, Last 3 RW: 0.10, 0.10 & 0.10 W: 61253.00, 61253.00 & 61253.00 Acc: 0.54, 0.54 & 0.54].\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(train_value_iterations):\n\u001b[0;32m     44\u001b[0m    states, _, _, returns, _ \u001b[39m=\u001b[39m exp_replay\u001b[39m.\u001b[39msample(rl_batch_size)\n\u001b[1;32m---> 45\u001b[0m    train_value_function(states\u001b[39m=\u001b[39;49mstates, returns\u001b[39m=\u001b[39;49mreturns)\n\u001b[0;32m     48\u001b[0m \u001b[39m# Clear replay data.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m exp_replay\u001b[39m.\u001b[39mreset()\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
            "File \u001b[1;32mc:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "np.seterr(divide='ignore', invalid='ignore')\n",
        "\n",
        "\n",
        "num_datasets = len(dataset_names)\n",
        "\n",
        "num_tests = (rl_iterations//test_frequency) + 1\n",
        "\n",
        "weights_history_tests = np.zeros(shape=(num_tests, num_datasets))\n",
        "acc_history_tests = np.zeros(shape=(num_tests, num_datasets))\n",
        "rw_history_tests = np.zeros(shape=(num_tests, num_datasets))\n",
        "test_counter = 1\n",
        "\n",
        "policy_losses = np.zeros(rl_iterations*train_policy_iterations, dtype=np.float32)\n",
        "\n",
        "for idx, env in enumerate(envs):\n",
        "    weights_history_tests[0, idx ] = env.weights_before\n",
        "    acc_history_tests[0, idx] = env.test_acc_before\n",
        "\n",
        "with tqdm(total=rl_iterations,\n",
        "      bar_format=\"{l_bar}{bar}|{n}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}, Last 3 RW: {postfix[0][0]:.2f}, {postfix[0][1]:.2f} & {postfix[0][2]:.2f} W: {postfix[1][0]:.2f}, {postfix[1][1]:.2f} & {postfix[1][2]:.2f} Acc: {postfix[2][0]:.2f}, {postfix[2][1]:.2f} & {postfix[2][2]:.2f}].\",\n",
        "      postfix=[\n",
        "         dict({0: 0, 1: 0, 2: np.mean(rw_history_tests[0])}),\n",
        "         dict({0: 0, 1: 0, 2: np.mean(acc_history_tests[0])}),\n",
        "         dict({0: 0, 1: 0, 2: np.mean(weights_history_tests[0])})]) as t:\n",
        "    \n",
        "   for i in range(1, rl_iterations+1):\n",
        "      for idx, env in enumerate(envs):\n",
        "         dataset_name = dataset_names[idx]\n",
        "         logger.info(f'Generating samples for dataset {dataset_name}.')\n",
        "         rw, acc, weights = play_and_record(env, run_id=run_id, test_number=i, dataset_name=dataset_name, save_name=exploration_filename, n_games=n_games_training, greedy=False,  save_replay=True)\n",
        "         logger.info(f'Training stats for dataset {dataset_name} were rw:{rw}, acc:{acc}, w:{weights}.')\n",
        "      \n",
        "      \n",
        "      # Train actor\n",
        "      for idx_policy_loss in range(train_policy_iterations):\n",
        "         states, actions, advantages, returns, logprobs = exp_replay.sample(rl_batch_size)\n",
        "         policy_loss, kl = train_policy(states=states, actions=actions, log_probabilities=logprobs, advantages=advantages)\n",
        "         policy_losses[i*train_policy_iterations + idx_policy_loss] = policy_loss\n",
        "         if kl > 1.5 * target_kl:\n",
        "            break\n",
        "      \n",
        "      # Train critic\n",
        "      for _ in range(train_value_iterations):\n",
        "         states, _, _, returns, _ = exp_replay.sample(rl_batch_size)\n",
        "         train_value_function(states=states, returns=returns)\n",
        "   \n",
        "\n",
        "      # Clear replay data.\n",
        "      exp_replay.reset()\n",
        "\n",
        "      if i % test_frequency == 0:\n",
        "         logger.info(f'Testing datasets.')\n",
        "         for idx, env in enumerate(envs):\n",
        "               dataset_name = dataset_names[idx]\n",
        "               logger.info(f'Testing agent for dataset {dataset_name}.')\n",
        "               rw, acc, weights = play_and_record(env, run_id=run_id, test_number=i, dataset_name=dataset_name,save_name=test_filename,n_games=n_games_testing, greedy=True, save_replay=False)\n",
        "               logger.info(f'Testing stats for dataset {dataset_name} were rw:{rw}, acc:{acc}, w:{weights}.')\n",
        "               rw_history_tests[test_counter, idx] = rw\n",
        "               acc_history_tests[test_counter, idx] = acc\n",
        "               weights_history_tests[test_counter, idx] = weights\n",
        "         \n",
        "         agent.actor.save_weights(agents_path+'actor.chkpt')\n",
        "         agent.critic.save_weights(agents_path+'critic.chkpt')\n",
        "\n",
        "         t.postfix[0][2] = np.mean(rw_history_tests[test_counter])\n",
        "            \n",
        "            \n",
        "         try:\n",
        "               t.postfix[0][1] = np.mean(rw_history_tests[test_counter-1])\n",
        "         except IndexError:\n",
        "               t.postfix[0][1] = 0\n",
        "         try:\n",
        "               t.postfix[0][0] =  np.mean(rw_history_tests[test_counter-2])\n",
        "         except IndexError:\n",
        "               t.postfix[0][0] = 0\n",
        "\n",
        "         t.postfix[1][2] = np.mean(weights_history_tests[test_counter])\n",
        "         try:\n",
        "               t.postfix[1][1] = np.mean(weights_history_tests[test_counter-1])\n",
        "         except IndexError:\n",
        "               t.postfix[1][1] = 0\n",
        "         try:\n",
        "               t.postfix[1][0] = np.mean(weights_history_tests[test_counter-2])\n",
        "         except IndexError:\n",
        "               t.postfix[1][0] = 0\n",
        "\n",
        "         t.postfix[2][2] = np.mean(acc_history_tests[test_counter])\n",
        "         try:\n",
        "               t.postfix[2][1] = np.mean(acc_history_tests[test_counter-1])\n",
        "         except IndexError:\n",
        "               t.postfix[2][1] = 0\n",
        "         try:\n",
        "               t.postfix[2][0] = np.mean(acc_history_tests[test_counter-2])\n",
        "         except IndexError:\n",
        "               t.postfix[2][0] = 0\n",
        "\n",
        "         \n",
        "                    \n",
        "         #clear_output(True)\n",
        "\n",
        "         test_counter += 1\n",
        "         fig = plt.figure(figsize=(12,6))\n",
        "         ax1 = fig.add_subplot(131)\n",
        "         ax2 = fig.add_subplot(132)\n",
        "         ax3 = fig.add_subplot(133)\n",
        "         ax1.title.set_text('Accuracy')\n",
        "         for idx, dataset_name in enumerate(dataset_names):\n",
        "               ax1.plot(acc_history_tests[:test_counter, idx])\n",
        "         ax1.legend(dataset_names)\n",
        "         ax2.title.set_text('Weights')\n",
        "         for idx, dataset_name in enumerate(dataset_names):\n",
        "               ax2.plot(weights_history_tests[:test_counter, idx])\n",
        "         ax2.legend(dataset_names)\n",
        "         ax3.title.set_text('Reward')\n",
        "         for idx, dataset_name in enumerate(dataset_names):\n",
        "               ax3.plot(rw_history_tests[:test_counter, idx])\n",
        "         ax3.legend(dataset_names)\n",
        "         plt.xlabel('Epochs')\n",
        "         plt.savefig(data_path + f'figures\\\\{agent_name}.png', dpi=1200)\n",
        "         plt.close()\n",
        "\n",
        "      t.update()\n",
        "    \n",
        "         "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "47aaa0ff824686579d53327e8b8be0a557935a1d134cd2eb3e64085ff0644594"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
