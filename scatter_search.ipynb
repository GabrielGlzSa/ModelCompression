{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from CompressionLibrary.utils import calculate_model_weights\n",
    "from CompressionLibrary.custom_layers import DenseSVD, MLPConv\n",
    "from CompressionLibrary.reward_functions import reward_MnasNet as calculate_reward\n",
    "from uuid import uuid4\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from CompressionLibrary.CompressionTechniques import InsertDenseSVD, MLPCompression\n",
    "import copy\n",
    "\n",
    "import gc\n",
    "\n",
    "from deap import base, creator\n",
    "from deap import algorithms\n",
    "import random\n",
    "from deap import tools\n",
    "\n",
    "\n",
    "current_os = 'windows'\n",
    "\n",
    "\n",
    "dataset_name = 'imagenet2012'\n",
    "batch_size = 32\n",
    "\n",
    "agent_name = 'scatter_search' + '_' + dataset_name\n",
    "\n",
    "\n",
    "if current_os == 'windows':\n",
    "    data_path = f'G:\\\\Python projects\\\\ModelCompressionRL\\\\data\\\\'\n",
    "    log_path = f'G:\\\\Python projects\\\\ModelCompressionRL\\\\data\\\\logs\\\\ModelCompression_{agent_name}.log'\n",
    "    exploration_filename = data_path + f'stats/{agent_name}_training.csv'\n",
    "    test_filename = data_path + f'stats\\\\{agent_name}_testing.csv'\n",
    "    figures_path = data_path+f'figures\\\\{agent_name}'\n",
    "    datasets_path = \"G:\\\\ImageNet 2012\\\\\"#data_path+\"datasets\"\n",
    "else:\n",
    "    data_path = './data'\n",
    "    log_path = f'/home/A00806415/DCC/ModelCompression/data/logs/ModelCompression_{agent_name}.log'\n",
    "    exploration_filename = data_path + f'/stats/{agent_name}_training.csv'\n",
    "    test_filename = data_path + f'/stats/{agent_name}_testing.csv'\n",
    "    figures_path = data_path+f'/figures/{agent_name}'\n",
    "\n",
    "\n",
    "# logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)\n",
    "logging.basicConfig(level=logging.DEBUG, handlers=[\n",
    "    logging.FileHandler(log_path, 'w+')],\n",
    "    format='%(asctime)s -%(levelname)s - %(funcName)s -  %(message)s')\n",
    "logging.root.setLevel(logging.DEBUG)\n",
    "\n",
    "log = logging.getLogger('tensorflow')\n",
    "log.setLevel(logging.ERROR)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# layer_name_list = ['conv2d_1',  'dense', 'dense_1']\n",
    "layer_name_list = [ 'block2_conv1', 'block2_conv2', \n",
    "                    'block3_conv1', 'block3_conv2', 'block3_conv3',\n",
    "                    'block4_conv1', 'block4_conv2', 'block4_conv3',\n",
    "                    'block5_conv1', 'block5_conv2', 'block5_conv3',\n",
    "                    'fc1', 'fc2']\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "def create_model(dataset_name, train_ds, valid_ds):\n",
    "    \n",
    "    train_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "\n",
    "    model = tf.keras.applications.vgg16.VGG16(\n",
    "                            include_top=True,\n",
    "                            weights='imagenet',\n",
    "                            input_shape=(224,224,3),\n",
    "                            classes=1000,\n",
    "                            classifier_activation='softmax'\n",
    "                        )\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss_object,\n",
    "                    metrics=train_metric)\n",
    "\n",
    "    return model    \n",
    "\n",
    "@tf.function\n",
    "def imagenet_preprocessing(img, label):\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img = tf.image.resize(img, size=(224,224), method='bicubic')\n",
    "    img = tf.keras.applications.vgg16.preprocess_input(img, data_format=None)\n",
    "    return img, label\n",
    "\n",
    "def load_dataset(dataset, batch_size=64):\n",
    "    splits, info = tfds.load('imagenet2012', as_supervised=True, with_info=True, shuffle_files=True, \n",
    "                                split=['validation', 'validation','validation'], data_dir=datasets_path)\n",
    "\n",
    "    #   splits, info = tfds.load('imagenet2012', as_supervised=True, with_info=True, shuffle_files=True, \n",
    "    #                               split=['train[:80%]', 'train[80%:]','validation'], data_dir=data_path)\n",
    "                                \n",
    "    (_, validation_examples, _) = splits\n",
    "    num_examples = info.splits['validation'].num_examples#info.splits['train'].num_examples\n",
    "\n",
    "    num_classes = info.features['label'].num_classes\n",
    "    input_shape = info.features['image'].shape\n",
    "\n",
    "    input_shape = (224,224,3)\n",
    "    valid_ds = validation_examples.map(imagenet_preprocessing, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return valid_ds, input_shape, num_classes\n",
    "\n",
    "def get_max_hidden_units(model, layer_list):\n",
    "    max_values = []\n",
    "    for layer_name in layer_list:\n",
    "        layer = model.get_layer(layer_name)\n",
    "        if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "            kernel, bias = layer.get_weights()\n",
    "            h, w, c, filters = kernel.shape\n",
    "            weights = tf.reshape(kernel, shape=[-1, filters])\n",
    "            input_size, _ = weights.shape\n",
    "            units = filters\n",
    "            \n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            weights, bias = layer.get_weights()\n",
    "            input_size , units = weights.shape\n",
    "            \n",
    "        max_hidden_units = (input_size * units)//(input_size+units)\n",
    "        max_values.append(max_hidden_units)\n",
    "\n",
    "    return max_values\n",
    "\n",
    "\n",
    "\n",
    "valid_ds, input_shape, _ = load_dataset(dataset_name, batch_size)\n",
    "train_ds = test_ds = valid_ds\n",
    "parameters = {}\n",
    "parameters['InsertDenseSVD'] = {'layer_name': None, 'percentage': None, 'hidden_units':None}\n",
    "parameters['MLPCompression'] = {'layer_name': None, 'percentage': None, 'hidden_units':None}\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "train_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "verbose = False\n",
    "\n",
    "# Create a the original model to calculate stats before.\n",
    "temp_model = create_model(dataset_name=dataset_name, train_ds=train_ds, valid_ds=valid_ds)\n",
    "weights_before = calculate_model_weights(temp_model)\n",
    "test_loss, test_acc_before = temp_model.evaluate(test_ds, verbose=verbose)\n",
    "val_loss, val_acc_before = temp_model.evaluate(valid_ds, verbose=verbose)\n",
    "\n",
    "\n",
    "max_hidden_units = get_max_hidden_units(temp_model, layer_name_list)\n",
    "del temp_model\n",
    "\n",
    "logging.info(f'Max hidden values are {max_hidden_units}.')\n",
    "\n",
    "logger.info(f'Max number of singular values per layer are : {max_hidden_units}.')\n",
    "\n",
    "eval_dict = dict()\n",
    "def evaluation_function(ind):\n",
    "    ind = fix_solution(ind)\n",
    "    callbacks = []\n",
    "    model = create_model(dataset_name=dataset_name, train_ds=train_ds, valid_ds=valid_ds)\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    if isinstance(model.layers[0], tf.keras.layers.InputLayer):\n",
    "        x = model.layers[1](inputs)\n",
    "        start = 2\n",
    "    else:\n",
    "        x = model.layers[0](inputs)\n",
    "        start = 1\n",
    "    for layer in model.layers[start:]:\n",
    "        if layer.name in layer_name_list:\n",
    "            weights, bias = layer.get_weights()\n",
    "            hidden_units = ind[layer_name_list.index(layer.name)]\n",
    "\n",
    "            if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "                config = layer.get_config()\n",
    "                filters = config['filters']\n",
    "                kernel_size = config['kernel_size']\n",
    "                activation = config['activation']\n",
    "                padding = config['padding']\n",
    "                weights = tf.reshape(weights, shape=[-1, filters])\n",
    "                \n",
    "                try:\n",
    "                    s, u, v = tf.linalg.svd(weights, full_matrices=False)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    with tf.device('/CPU:0'):\n",
    "                        s, u, v = tf.linalg.svd(weights, full_matrices=False)\n",
    "\n",
    "                u = tf.slice(u, begin=[0, 0], size=[weights.shape[0], hidden_units])\n",
    "                s = tf.slice(s, begin=[0], size=[hidden_units])\n",
    "\n",
    "                # Transpose V as V is returned as V^T.\n",
    "                v = tf.slice(tf.linalg.matrix_transpose(v), begin=[0, 0], size=[hidden_units, filters])\n",
    "                n = tf.matmul(tf.linalg.diag(s), v)\n",
    "\n",
    "                new_layer = MLPConv(filters=filters, hidden_units=hidden_units, kernel_size=kernel_size, padding=padding,\n",
    "                  activation=activation,\n",
    "                  name=layer.name + '/MLPConv')\n",
    "                \n",
    "                new_layer(layer.input)\n",
    "\n",
    "                new_layer.set_weights([u, n, bias])\n",
    "\n",
    "            elif isinstance(layer, tf.keras.layers.Dense):\n",
    "                _ , units = weights.shape\n",
    "                activation = layer.get_config()['activation']\n",
    "                try:\n",
    "                    s, u, v = tf.linalg.svd(weights, full_matrices=False)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    with tf.device('/CPU:0'):\n",
    "                        s, u, v = tf.linalg.svd(weights, full_matrices=False)\n",
    "\n",
    "                u = tf.slice(u, begin=[0, 0], size=[weights.shape[0], hidden_units])\n",
    "                s = tf.slice(s, begin=[0], size=[hidden_units])\n",
    "\n",
    "                # Transpose V as V is returned as V^T.\n",
    "                v = tf.slice(tf.linalg.matrix_transpose(v), begin=[0, 0], size=[hidden_units, units])\n",
    "                n = tf.matmul(tf.linalg.diag(s), v)\n",
    "                new_layer = DenseSVD(units=units, hidden_units=hidden_units,\n",
    "                    activation=activation,\n",
    "                    name=layer.name + '/DenseSVD')\n",
    "                new_layer(layer.input)\n",
    "\n",
    "                new_layer.set_weights([u.numpy(), n, bias])\n",
    "                            \n",
    "            else:\n",
    "                raise f'Layer {layer.name} has an unsupported layer type for compression.'\n",
    "            \n",
    "            x = new_layer(x)\n",
    "        else:\n",
    "            x = layer(x)\n",
    "       \n",
    "    model = tf.keras.Model(inputs, x)\n",
    "    \n",
    "    model.compile(optimizer, loss_object)\n",
    "    seq_key = ','.join(map(str,ind))\n",
    "    if seq_key not in eval_dict.keys():\n",
    "\n",
    "        logger.debug('Evaluating model because it has not been explored before.')\n",
    "        start = datetime.now()\n",
    "        test_loss, test_acc_after = model.evaluate(test_ds, verbose=verbose)\n",
    "        val_loss, val_acc_after = test_loss, test_acc_after #model.evaluate(valid_ds, verbose=verbose)\n",
    "        end = datetime.now()\n",
    "        logger.debug(f'Evaluation took {(end-start).total_seconds():2f} seconds. ')\n",
    "    \n",
    "    else:\n",
    "        test_acc_after, weights_after = eval_dict[seq_key]\n",
    "        logger.debug(f'Found evaluation of {ind}: ({test_acc_after}, {weights_after})')\n",
    "    weights_after = calculate_model_weights(model)\n",
    "    stats = {\n",
    "                'weights_before': weights_before, \n",
    "                'weights_after': weights_after, \n",
    "                'accuracy_after': test_acc_after, \n",
    "                'accuracy_before': test_acc_before}\n",
    "\n",
    "    reward = calculate_reward(stats)\n",
    "\n",
    "    eval_dict[seq_key] = (stats['accuracy_after'], stats['weights_after'])\n",
    "\n",
    "    logger.debug(f'Solution {ind} has {weights_after} weights and an accuracy of {test_acc_after}. Reward is {reward}')\n",
    "    del model\n",
    "    gc.collect()\n",
    "    return stats['accuracy_after'], stats['weights_after']\n",
    "\n",
    "\n",
    "def mutation(ind, indpb, max_delta=10):\n",
    "    for action_idx in range(len(ind)):\n",
    "        if random.uniform(0.0, 1.0) < indpb:\n",
    "            delta = np.random.randint(low=-max_delta, high=max_delta)\n",
    "            ind[action_idx] = np.clip(ind[action_idx] + delta, a_min=1, a_max=max_hidden_units[action_idx]+1)\n",
    "    return ind,\n",
    "\n",
    "\n",
    "def fix_solution(ind):\n",
    "    for action_idx in range(len(ind)):\n",
    "        ind[action_idx] = np.clip(ind[action_idx], 1, max_hidden_units[action_idx]+1)\n",
    "    return ind\n",
    "\n",
    "\n",
    "def div_gen_method(n, max_hidden_units):\n",
    "    n_variables = len(max_hidden_units)\n",
    "    subranges = 4\n",
    "    freq = np.ones(shape=(n_variables, subranges))\n",
    "    \n",
    "    population = []\n",
    "    for _ in range(n):\n",
    "        new_sol = []\n",
    "        for j, max_value in enumerate(max_hidden_units):\n",
    "            # Add one to consider no compression.\n",
    "            subrange_size = (max_value+1) //4\n",
    "            p = freq[j]/np.sum(freq[j])\n",
    "            chosen_subrange = np.random.choice(subranges,size=1, replace=False, p=p)[0]\n",
    "            start = subrange_size * chosen_subrange\n",
    "            end = subrange_size * (chosen_subrange+1)\n",
    "            new_sol.append(random.randint(start, end))\n",
    "            \n",
    "            freq[j, chosen_subrange] += 1\n",
    "        population.append(new_sol)\n",
    "        print(new_sol)\n",
    "    return population\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "div_gen_method(10, max_hidden_units)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the refSet.\n",
    "b = 20\n",
    "# Initial number of high-quality solutions in the refSet.\n",
    "b1 = 10\n",
    "# Initial number of diverse solutions in refSet.\n",
    "b2 = 10\n",
    "\n",
    "\n",
    "\n",
    "class Solution:\n",
    "    def __init__(self, x, evaluations=None) -> None:\n",
    "        self.x = x\n",
    "        self.evaluations = evaluations\n",
    "\n",
    "\n",
    "class ReferenceSet:\n",
    "    def __init__(self, size=0) -> None:\n",
    "        self.solutions = []\n",
    "        self.size = size\n",
    "        self.newSol = False\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "def div_gen_method(n, max_hidden_units):\n",
    "    n_variables = len(max_hidden_units)\n",
    "    subranges = 4\n",
    "    freq = np.ones(shape=(n_variables, subranges))\n",
    "    \n",
    "    population = []\n",
    "    for _ in range(n):\n",
    "        new_sol = []\n",
    "        for j, max_value in enumerate(max_hidden_units):\n",
    "            # Add one to consider no compression.\n",
    "            subrange_size = (max_value+1) //4\n",
    "            p = (1 + np.max(freq[j]) - freq[j])    \n",
    "            p = p / np.sum(p)\n",
    "            chosen_subrange = np.random.choice(subranges,size=1, replace=False, p=p)[0]\n",
    "            start = subrange_size * chosen_subrange\n",
    "            end = subrange_size * (chosen_subrange+1)\n",
    "            new_sol.append(random.randint(start, end))\n",
    "            \n",
    "            freq[j, chosen_subrange] += 1\n",
    "        population.append(new_sol)\n",
    "\n",
    "    return population\n",
    "\n",
    "\n",
    "\n",
    "def impove_method(sol):\n",
    "\n",
    "\n",
    "    return sol\n",
    "\n",
    "\n",
    "def update_reference_set_method(refset):\n",
    "    pass\n",
    "\n",
    "\n",
    "def subset_generation_method(refset):\n",
    "    pass\n",
    "\n",
    "def solution_combination_method(sol1, sol2):\n",
    "    pass\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
