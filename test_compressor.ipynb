{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import logging\n",
    "from uuid import uuid4\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from CompressionLibrary.agent_evaluators import make_env_imagenet, evaluate_agents\n",
    "from CompressionLibrary.reinforcement_models import DuelingDQNAgent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(1000))\n",
    "r = [0.999, 0.995, 0.99, 0.95, 0.92, 0.9]\n",
    "for v in r:\n",
    "    f = lambda a: 0.9*v**a\n",
    "    y = list(map(f,x))\n",
    "    plt.plot(x,y)\n",
    "\n",
    "legends = list(map(str, r))\n",
    "plt.legend(legends)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents_names = map(lambda x: 'DuelingDQN_'+x, ['mnist-kmnist', 'fashion_mnist-kmnist', 'mnist-fashion_mnist'])\n",
    "datasets= ['fashion_mnist', 'kmnist', 'mnist']\n",
    "run_id = datetime.now().strftime('%Y-%m-%d-%H-%M%S-') + str(uuid4())\n",
    "\n",
    "try:\n",
    "  # Use below for TPU\n",
    "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')\n",
    "  tf.config.experimental_connect_to_cluster(resolver)\n",
    "  # This is the TPU initialization code that has to be at the beginning.\n",
    "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "  print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "  strategy = tf.distribute.TPUStrategy(resolver)\n",
    "  data_path = '/mnt/disks/mcdata/data'\n",
    "\n",
    "except:\n",
    "  print('ERROR: Not connected to a TPU runtime; Using GPU strategy instead!')\n",
    "  strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "  data_path = './data'\n",
    "  \n",
    "if strategy:\n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, handlers=[\n",
    "    logging.FileHandler(f'/home/A00806415/DCC/ModelCompression/data/ModelCompression_DDQN_tests.log', 'w+')],\n",
    "    format='%(asctime)s -%(levelname)s - %(funcName)s -  %(message)s')\n",
    "logging.root.setLevel(logging.DEBUG)\n",
    "\n",
    "log = logging.getLogger('tensorflow')\n",
    "log.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "current_state = 'layer_input'\n",
    "next_state = 'layer_output'\n",
    "\n",
    "epsilon_start_value = 0.9\n",
    "verbose = 0\n",
    "eval_n_samples = 10\n",
    "n_samples_mode = -1\n",
    "tuning_batch_size = 256\n",
    "tuning_epochs = 30\n",
    "\n",
    "layer_name_list = ['conv2d_1',  'dense', 'dense_1']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation and data loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preprocessing(img, label):\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img = img/255.0\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in datasets:\n",
    "    def create_model():\n",
    "        checkpoint_path = f\"./data/models/lenet_{dataset_name}/cp.ckpt\"\n",
    "        optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "        loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        train_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        input = tf.keras.layers.Input((28,28,1))\n",
    "        x = tf.keras.layers.Conv2D(6, (5,5), padding='SAME', activation='sigmoid', name='conv2d')(input)\n",
    "        x = tf.keras.layers.AveragePooling2D((2,2), strides=2, name='avg_pool_1')(x)\n",
    "        x = tf.keras.layers.Conv2D(16, (5,5), padding='VALID', activation='sigmoid', name='conv2d_1')(x)\n",
    "        x = tf.keras.layers.AveragePooling2D((2,2), strides=2, name='avg_pool_2')(x)\n",
    "        x = tf.keras.layers.Flatten(name='flatten')(x)\n",
    "        x = tf.keras.layers.Dense(120, activation='sigmoid', name='dense')(x)\n",
    "        x = tf.keras.layers.Dense(84, activation='sigmoid', name='dense_1')(x)\n",
    "        x = tf.keras.layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "\n",
    "        model = tf.keras.Model(input, x, name='LeNet')\n",
    "        model.compile(optimizer=optimizer, loss=loss_object,\n",
    "                        metrics=[train_metric])\n",
    "\n",
    "        try:\n",
    "            model.load_weights(checkpoint_path).expect_partial()\n",
    "        except:\n",
    "            cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=True,\n",
    "                                                    save_weights_only=True,\n",
    "                                                    verbose=1)\n",
    "            model.fit(train_ds,\n",
    "            epochs=3000,\n",
    "            validation_data=valid_ds,\n",
    "            callbacks=[cp_callback])\n",
    "\n",
    "        return model       \n",
    "\n",
    "\n",
    "    def load_dataset(batch_size=128):\n",
    "        splits, info = tfds.load(dataset_name, as_supervised=True, with_info=True, shuffle_files=True, \n",
    "                                    split=['train[:80%]', 'train[80%:]','test'])\n",
    "\n",
    "        (train_examples, validation_examples, test_examples) = splits\n",
    "        num_examples = info.splits['train'].num_examples\n",
    "\n",
    "        num_classes = info.features['label'].num_classes\n",
    "        input_shape = info.features['image'].shape\n",
    "\n",
    "        input_shape = (28,28,1)\n",
    "\n",
    "        train_ds = train_examples.map(dataset_preprocessing, num_parallel_calls=tf.data.AUTOTUNE).shuffle(buffer_size=1000, reshuffle_each_iteration=True).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        valid_ds = validation_examples.map(dataset_preprocessing, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "        test_ds = test_examples.map(dataset_preprocessing, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        return train_ds, valid_ds, test_ds, input_shape, num_classes\n",
    "\n",
    "    train_ds, valid_ds, test_ds, input_shape, _ = load_dataset(tuning_batch_size)\n",
    "    input_shape = (28,28,1)\n",
    "    env = make_env_imagenet(\n",
    "                create_model=create_model, \n",
    "                train_ds=train_ds, \n",
    "                valid_ds=valid_ds, \n",
    "                test_ds=test_ds, \n",
    "                input_shape=input_shape, \n",
    "                layer_name_list=layer_name_list, \n",
    "                num_feature_maps=n_samples_mode, \n",
    "                tuning_batch_size=tuning_batch_size, \n",
    "                tuning_epochs=tuning_epochs, \n",
    "                verbose=verbose, \n",
    "                tuning_mode='layer', \n",
    "                current_state_source=current_state, \n",
    "                next_state_source=next_state, \n",
    "                strategy=strategy, \n",
    "                model_path=data_path)\n",
    "    conv_shape, dense_shape = env.observation_space()\n",
    "    conv_n_actions, fc_n_actions = env.action_space()\n",
    "\n",
    "    def load_weigths_into_target_network(agent, target_network):\n",
    "        \"\"\" assign target_network.weights variables to their respective agent.weights values. \"\"\"\n",
    "        for i in range(len(agent.model.layers)):\n",
    "            target_network.model.layers[i].set_weights(\n",
    "                agent.model.layers[i].get_weights())\n",
    "\n",
    "\n",
    "    with strategy.scope():\n",
    "        \n",
    "        fc_agent = DuelingDQNAgent(name=\"ddqn_agent_fc\", state_shape=dense_shape,\n",
    "                            n_actions=fc_n_actions, epsilon=epsilon_start_value, layer_type='fc')\n",
    "\n",
    "        conv_agent = DuelingDQNAgent(\n",
    "            name=\"ddqn_agent_conv\", state_shape=conv_shape, n_actions=conv_n_actions, epsilon=epsilon_start_value, layer_type='cnn')\n",
    "\n",
    "        for agent_name in agents_names:\n",
    "            print(dataset_name, agent_name)\n",
    "            conv_agent.model.load_weights(\n",
    "                data_path+'/checkpoints/{}_my_checkpoint_conv_agent'.format(agent_name))\n",
    "            fc_agent.model.load_weights(\n",
    "                data_path+'/checkpoints/{}_my_checkpoint_fc_agent'.format(agent_name))\n",
    "\n",
    "            rw, acc, weights = evaluate_agents(env, conv_agent, fc_agent,run_id=run_id,test_number=agent_name, dataset_name=dataset_name,save_name=data_path+'/DuelingDQN_test_unseen.csv', n_games=eval_n_samples)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('mc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51ccaed8c3f743b1f0f573d1bf9b420e6968f0ad98d33b49a16252e93b5cad05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
