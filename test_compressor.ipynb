{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabri\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import logging\n",
    "from uuid import uuid4\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "\n",
    "from CompressionLibrary.agent_evaluators import make_env_adadeep, evaluate_agents\n",
    "from CompressionLibrary.reinforcement_models import DuelingDQNAgent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "agents_names = list(map(lambda x: 'LeNet_DDDQN_MKII_'+x, ['fashion_mnist','kmnist', 'mnist', 'fashion_mnist-kmnist', 'fashion_mnist-mnist','kmnist-mnist', 'fashion_mnist-kmnist-mnist']))\n",
    "dataset_names = ['fashion_mnist','kmnist', 'mnist']\n",
    "run_id = datetime.now().strftime('%Y-%m-%d-%H-%M%S-') + str(uuid4())\n",
    "\n",
    "\n",
    "strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "data_path = \"G:\\\\Python project\\\\MODEL COMPRESSION\\\\ModelCompressionRL\\\\data\\\\\"\n",
    "\n",
    "log_name = 'test_agents_training_ds_12k'\n",
    "test_filename = data_path + 'stats\\\\DDDQN_MKII_{}_testing.csv'.format(log_name)\n",
    "\n",
    "agents_path = data_path+'agents\\\\DDDQN\\\\checkpoints\\\\'\n",
    "\n",
    "if strategy:\n",
    "    print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, handlers=[\n",
    "    logging.FileHandler(data_path + f'logs\\\\{log_name}.log', 'w+')],\n",
    "    format='%(asctime)s -%(levelname)s - %(funcName)s -  %(message)s')\n",
    "logging.root.setLevel(logging.DEBUG)\n",
    "\n",
    "log = logging.getLogger('tensorflow')\n",
    "log.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "current_state = 'layer_input'\n",
    "next_state = 'layer_output'\n",
    "\n",
    "epsilon_start_value = 0.9\n",
    "verbose = 0\n",
    "eval_n_samples = 4\n",
    "n_samples_mode = 12000\n",
    "tuning_batch_size = 256\n",
    "tuning_epochs = 30\n",
    "\n",
    "layer_name_list = ['conv2d_1',  'dense', 'dense_1']\n",
    "\n",
    "\n",
    "def calculate_reward(stats: dict) -> float:\n",
    "   return 1 - (stats['weights_after']/stats['weights_before']) + stats['accuracy_after'] - 0.9 * stats['accuracy_before']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation and data loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 28, 6)         156       \n",
      "                                                                 \n",
      " avg_pool_1 (AveragePooling2  (None, 14, 14, 6)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 10, 10, 16)        2416      \n",
      "                                                                 \n",
      " avg_pool_2 (AveragePooling2  (None, 5, 5, 16)         0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 400)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 120)               48120     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61,706\n",
      "Trainable params: 61,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3 envs\n"
     ]
    }
   ],
   "source": [
    "def create_model(dataset_name, train_ds, valid_ds):\n",
    "    checkpoint_path = data_path+ f\"models\\\\lenet_{dataset_name}\\\\cp.ckpt\"\n",
    "    optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    train_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    input = tf.keras.layers.Input((28,28,1))\n",
    "    x = tf.keras.layers.Conv2D(6, (5,5), padding='SAME', activation='sigmoid', name='conv2d')(input)\n",
    "    x = tf.keras.layers.AveragePooling2D((2,2), strides=2, name='avg_pool_1')(x)\n",
    "    x = tf.keras.layers.Conv2D(16, (5,5), padding='VALID', activation='sigmoid', name='conv2d_1')(x)\n",
    "    x = tf.keras.layers.AveragePooling2D((2,2), strides=2, name='avg_pool_2')(x)\n",
    "    x = tf.keras.layers.Flatten(name='flatten')(x)\n",
    "    x = tf.keras.layers.Dense(120, activation='sigmoid', name='dense')(x)\n",
    "    x = tf.keras.layers.Dense(84, activation='sigmoid', name='dense_1')(x)\n",
    "    x = tf.keras.layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "\n",
    "    model = tf.keras.Model(input, x, name='LeNet')\n",
    "    model.compile(optimizer=optimizer, loss=loss_object,\n",
    "                    metrics=[train_metric])\n",
    "\n",
    "    try:\n",
    "        model.load_weights(checkpoint_path).expect_partial()\n",
    "    except:\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=True,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "        model.fit(train_ds,\n",
    "          epochs=3000,\n",
    "          validation_data=valid_ds,\n",
    "          callbacks=[cp_callback])\n",
    "\n",
    "    return model       \n",
    "\n",
    "def dataset_preprocessing(img, label):\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    img = img/255.0\n",
    "    return img, label\n",
    "\n",
    "def load_dataset(dataset_name, batch_size=128):\n",
    "    splits, info = tfds.load(dataset_name, as_supervised=True, with_info=True, shuffle_files=True, \n",
    "                                split=['train[:80%]', 'train[80%:]','test'])\n",
    "\n",
    "    (train_examples, validation_examples, test_examples) = splits\n",
    "    num_examples = info.splits['train'].num_examples\n",
    "\n",
    "    num_classes = info.features['label'].num_classes\n",
    "    input_shape = info.features['image'].shape\n",
    "\n",
    "    input_shape = (28,28,1)\n",
    "\n",
    "    train_ds = train_examples.map(dataset_preprocessing, num_parallel_calls=tf.data.AUTOTUNE).shuffle(buffer_size=1000, reshuffle_each_iteration=True).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    valid_ds = validation_examples.map(dataset_preprocessing, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds = test_examples.map(dataset_preprocessing, num_parallel_calls=tf.data.AUTOTUNE).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_ds, valid_ds, test_ds, input_shape, num_classes\n",
    "\n",
    "\n",
    "input_shape = (28,28,1)\n",
    "\n",
    "def create_environments(dataset_names):\n",
    "    environments = []\n",
    "    for dataset in dataset_names:\n",
    "        train_ds, valid_ds, test_ds, input_shape, _ = load_dataset(dataset, tuning_batch_size)\n",
    "        new_func = partial(create_model, dataset_name=dataset, train_ds=train_ds, valid_ds=valid_ds)\n",
    "        env = make_env_adadeep(\n",
    "            create_model=new_func, \n",
    "            reward_func=calculate_reward,\n",
    "            train_ds=train_ds, \n",
    "            valid_ds=valid_ds, \n",
    "            test_ds=test_ds, \n",
    "            input_shape=input_shape, \n",
    "            layer_name_list=layer_name_list, \n",
    "            num_feature_maps=n_samples_mode, \n",
    "            tuning_batch_size=tuning_batch_size, \n",
    "            tuning_epochs=tuning_epochs, \n",
    "            verbose=verbose, \n",
    "            tuning_mode='layer',\n",
    "            get_state_from='train',\n",
    "            current_state_source=current_state, \n",
    "            next_state_source=next_state, \n",
    "            strategy=strategy, \n",
    "            model_path=data_path)\n",
    "\n",
    "        environments.append(env)\n",
    "\n",
    "    return environments\n",
    "\n",
    "envs = create_environments(dataset_names)\n",
    "envs[0].model.summary()\n",
    "print(f'{len(envs)} envs')\n",
    "\n",
    "conv_shape, dense_shape = envs[0].observation_space()\n",
    "conv_n_actions, fc_n_actions = envs[0].action_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "fc_agent = DuelingDQNAgent(name=\"ddqn_agent_fc\", state_shape=dense_shape,\n",
    "                    n_actions=fc_n_actions, epsilon=epsilon_start_value, layer_type='fc')\n",
    "\n",
    "conv_agent = DuelingDQNAgent(\n",
    "    name=\"ddqn_agent_conv\", state_shape=conv_shape, n_actions=conv_n_actions, epsilon=epsilon_start_value, layer_type='cnn')\n",
    "\n",
    "iterations = len(dataset_names) * len(agents_names)\n",
    "with tqdm(total=iterations) as t:\n",
    "    for idx, dataset_name in enumerate(dataset_names):\n",
    "        env = envs[idx]\n",
    "\n",
    "        for agent_name in agents_names:\n",
    "            # print(dataset_name, agent_name)\n",
    "            conv_agent.model.load_weights(agents_path+agent_name+'_conv_agent.ckpt')\n",
    "            fc_agent.model.load_weights(agents_path+agent_name+'_fc_agent.ckpt')\n",
    "\n",
    "            rw, acc, weights = evaluate_agents(env, conv_agent, fc_agent,run_id=run_id,test_number=agent_name, dataset_name=dataset_name, agent_name=agent_name,save_name=test_filename, n_games=eval_n_samples)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('mc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51ccaed8c3f743b1f0f573d1bf9b420e6968f0ad98d33b49a16252e93b5cad05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
